워드임베딩

단어가 가지는 특성을 덴스벡터로 표현.

세로 : 단어 index
가로 : Win. 개별 특성

Contextualized Word Embedding
문장이 FE에 입력으로 들어감

E.V F.E 분류기

나는 어제 밤을 먹었다.

빈도수->워드임베딩(단어의 특성. 동음이의어는 잘 구분 못함)->문맥적워드임베딩(문장).

어제꺼
-----------------------------
Sequencial Data
순서가 중요한거.
시간의 흐름에 따라 어떻게 바뀌는지 보려면, 데이터 하나만보면 모르고 그 주변것들도 봐야됨.
문맥이 중요.

RNN

토큰->임베딩->RNN->hidden state
앞에 뭐가 있었는지 알아야함.
이 영화 정말 재미있다.
재미 앞에 이 영화 정말도 같이 넣어줘야함.
앞 토큰 RNN을 돌린거(hidden state)를 뒤 RNN에 입력으로 같이 넣어줘야함.
정말은 영화으 ㅣ특성도 가짐.
재미있다에는 이 영화 정말에 대한 특성값도 들어가있음.
그래서 재미있다만 추론하는 애한테 넣어줌

문장별로 RNN 모델은 동일. 

hidden state:이전 기억
이런 모대메모리 시스템 이라고 함

x:현재타임스텝
h:이전타임스텝 처리 결과

RNN은 그라디언트배니싱에 취약해서 tanh를 씀

**

Auto regression?

개체명(명사) 인식

seq2seq 방식 설명
m2o - o2m 매니투원이랑 원투매니가 연결
앞에가 인코더 뒤에가 디코더

Pytorch RNN layer
hidden_size:히든사이즈는 보통 1개인데, 여러개도 지정할 수 있음. 여러개를 특성값으로 사용하겠다.

Batch_first : 디폴트는 False. 
원래 True로 하면(배치, 시퀀스길이, 피쳐수) 인데, (시퀀스길이, 배치, 피쳐수) 로 나온다.

LSTM Gate
Gate:어떤 값을 얼마나 통과시킬지 결정하는거.
메모리시스템의 문제:
Forget Gate : 오래된거부터 없애는게 아니고, 안중요한거부터 없앰. 곱셈을 통해 몇프로 기억만 남길지 정함.
Input Gate : 
Cell State update:지금 입력에 대한 처리결과를 반영. 그래디언트 소실 문제를 해결해줌.

GRU:위 세개 기능을 넣으면서 연산량을 줄임.


