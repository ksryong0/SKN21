체인으로 연결된 거를 하나의 workflow라고 보면,
workflow끼리도 연결 가능.
체인끼리 연결.

LCEL : | 로 묶어준거. 출력이 입력으로 들어감.
단점:순차적 흐름밖에 안됨
->제어나 반복을 하려면 함수나, Langgraph 이용.

workflow:사람이 지정해둔 일의 흐름. 체인.

RunnableLambda(함수 객체)
@chain decorator : 함수->RunnableLambda
LCEL에 함수를 포함시키면 RunnableLambda로 자동 변환

-------------------------이거 다시한번 복습에 정리할것!!-----------------------------------
체인끼리 연결해서 다른 체인을 만듬.
체인 = 레시피 체인 | 번역 체인
음식종류->레시피체인->결과를 번역. 언어도 입력으로 들어감.

입력값 관련 문제점!
레시피체인의 입력과 번역체인의 입력이 다름.
체인.invoke(언어,요리) 로 넣으면
언어, 요리가 첫번째 체인이 다 소모해버리고
두번째 체인의 입력은 첫번째 체인의 결과라서, 두번째 체인은 언어 입력을 못받음.

러너블 패러럴, 러너블 패스쓰루를 이때 사용
체인={
content:레시피체인,
language:itemgetter("language")
} | 번역 체인
체인.invoke("food":"찌개", "language":"영어"})
-------------------------------------------------------------------
함수 위에다 @chain 해주면 간단히 러너블로 바뀜.
제어문, 반복문이 필요할 때 함수로 구현하고 위에 @chain을 붙여줌.

set_llm_cache 함수를쓰면 똑같은 질문이 들어왔을 때, 캐시에 저장된 응답을 줌.

langsmith? 작업내역이 랭체인 사이트에 기록?


---------------------------복습-------------------

랭스미스.com

뉴 프로젝트
api 키 생성
3번 복사
export 제거. openai_api_key 제거.
LANGSMITH_PROJECT 값을 SKN21로 변경.
----------------------------------------------------
랭체인 메모리 기능

llm은 보통 stateless하다. 사용자의 상태를 유지해주지 않는다.
대화가 앞에있는 내용과 관련된게 있으면, 사람은 아까 얘기한거 라는 식으로 얘기함. 하지만 LLM은 이전 대화 내용을 들고있지 않음. 사용자가 누군지도 모름.

대화이력이 많아지면 현재질문과 관련없는 내용이 많아지고, 이것이 노이즈가 되어 LLM이 부정확한 응답을 할 수 있다. 이것땜에 입력을 못 받을 수도 있다.
비용문제.

확장 파일SQLite Viewer 설치

너무 긴 문자열은 괄호로 묶으면 됨.
\로 묶을수도 있음. 코드에서도 적용 가능
"a\
b\
c"

메세지 요약, 줄여주는 Runnable이 필요. 안그러면 너무 다 쌓임.
1. 요약->LLM
2. 줄이기 100->50
1 방식 + 2 방식: 최근 10건은 그대로 보내고, 이전은 요약

RAG(Retrieval Augmented Generation. 검색 증강 생성)
LLM의 문제점
1. 할루시네이션:모르는데 아는척. 거짓말.
2. 회사 기밀 문서는 LLM이 학습할 수 없음. 


해결방법
1. 파인튜닝
-> Knowledge Cutoff. 학습하는데도 오래걸림. 회사기밀은 학습시킬수도 없음. 직접 공부하는거랑 비슷.
2. 질문이 왔을 때 답이 포함된 컨텍스트를 건네줌.
질문				LLM
2025년 10월 		2025년 1워 1

(질문+답이있는 문서)를 프롬프트에 붙여서 LLM에게 보냄.
1. 검색(DB):주제와 관련된 문서들을 모아서, 잘 정제해서, DB에 저장.
2. 질문+Context(답이 있을거 같은 문서) (증강)
3. 답변(생성)

파인튜닝이 잘 만들면 성능은 좋음. 시간과 돈이 너무 많이 듬. 시간이 지나면 knowledge cutoff가 또 발생함.
RAG는 뭐가 문제냐? 잘못된 문서가 넘어오면 답을 못해줌. 검색을 얼마나 잘하냐가 문제. 검색 시스템 구축이 필요. RAG를 구현하는건 쉬운데, 잘하는게 어려움.

의미기반(임베딩)으로 검색해야함.
Feature Extractor->Feature Vector
질문 FV	A FV
		B FV
		C FV

임베딩 모델:질문 FV와 다른 FV간의 유사도를 계산하여 높은걸 사용하도록 학습시킴.

코사인 유사도

벡터 데이터베이스:데이터를 벡터간의 유사도 계산하여 찾을 수 있도록. 유사도는 KNN방식을 변형한 방식을 이용. 거리기반일 때는 유클리디안 디스턴스.

문서를 가져와서, 문서를 잘 나눠야 함.

청킹한 거를 임베딩 모델에 입력으로 넣으면 임베딩 벡터가 나옴.

질문(질문도 임베딩하고)->벡터디비에서 검색->질문+Context에 대한 답변-> 생성
			(질문 + 문서)
                               
도큐먼트 로더
다양한 애들을 동일한 패턴으로 읽어올 수 있다.
메타데이터
