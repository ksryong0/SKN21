문맥적 워드 임베딩

가로:E.V 크기
세로:단어 갯수


임베딩벡터(특성을 숫자로 나타냄)->RNN(Contextualized Word Embedding)->추론기
	         					문맥에 맞게 바뀜

양방향 RNN:성능은 더 좋음. 계산량은 많아짐

Auto Regressive:이전 스텝의 결과를 다음 스텝에 넣어줌.

LSTM gate
Forget
Input
Output 셀스테이트에서 현재 출력을 얼마나 빼줄지.

LSTM 단점
연산량이 너무 많다.
------------------------------
GRU
forget, input역할 : reset?

tanh:비선형성 넣어줌

GRU컨셉:
히든스테이트
현재 입력을 바탕으로해서 이전 히든스테이트들에 가중치를 곱해서 처리.
update gate:x5입력일때 h1, h2, h3, h4에 0.3을 곱해주기로 했으면 x5에는 0.7을 곱해서 추가함.
reset gate:현재타임스텝 ㅊㅓ리할떄, 이전까지의 히든스테이트를 얼마나 반영할지 결정. 위에 이전스테이트들에 몇을 곱할지 정하는 애

코드적으로 SimpleRNN하고 사용법은 같다.
