일반화 모델을 만드는게 우리 목적

오버피팅
traindata set은 이상치까지 맞추려 함
실제 새로운 데이터에서 정상적인 데이터 패턴을 맞추지 못함

해결방법:데이터양 늘리기.
1. 모델 변경
	모델을 다른거로
	모델을 복잡도(하이퍼파라미터) 변경

모델 복잡도에 따른 성능 변화
오버피팅:복잡도 높다
언더피팅:복잡도 낮다

모델 (하이퍼파라미터) 튜닝: 모델의 성능을 높이기 위해서 하이퍼파라미터를 변경해가면서 테스트. 후보군을 넣어보면서 성능 개선을 확인. 
1. 그리드 서치 이용 하이퍼파라미터 튜닝 자동화:이것도 자동화. 단순무식하게 
 바꿔보는거라서, 자동화가 가능. 모든 후보들에 대해 테스트.
2. 랜덤화서치 이용 튜닝 자동화:임의의 몇개의 조합만 테스트.

파이프라인
전처리, 모델 학습 과정을 객체에 넣어서 한번에 처리!
아니면 여러 객체의 전처리(결측치 처리 등)를 한번에 하거나, 여러 모델링을 한번에, 아니면 평가를 한번에.

컬럼트랜스포머
Transformer(변환기) : 전처리. 
SS, MMS, OHE, PCA는 fit, transform 함수를 공통적을 사용할 수 있다.

Estimator(추정(추론)기) : 머신러닝 모델.
DT RF SVC
fit, predict


어제 공부한 내용임!
--------------------------
Todo

# 컬럼트랜스포머에 전처리하는게 하나면 이렇게 컬럼명을 써줘도 됨. ndarray로 나오기때문에 결국 인덱스 번호가 필요. 결론은 인덱스 번호로 하는게 나음.

07_지도학습_SVM

SVM(Support Vector Machine):가장 거리를 멀게 만드는 결정경계를 찾는 머신.
거리가 가장 길게 만드는 선이 얘네 둘을 잘 분류한 선임.
결정경계:네모인지 삼각형인지 결정하는 선. 거리를 margin width 라고 함.
데이터가 이상치도 있고, 경계선 근처에 데이터가 섞여있는 경우도 있음.
규제 파라미터?
소프트마진 : 무조건 다 맞게 선을 설정.
하드 : 아웃라이어는 무시.
---------------------
마진 설정 하이퍼파라미터 C

C가 크면 클수록 규제를 약하게 함. 복잡도가 올라간다. 너무 규제가 약하면 overfitting
C가 작으면 규제를 강하게 한다. 너무 규제가 강하면 underfitting

Underfitting이 발생하면 규제를 약하게 한다.
규제를 약하게한다 == 모델의 복잡도를 높인다.

Overfitting일 때 규제를 강하게 한다.
규제를 강하게 한다는건 모델의 복잡도를 낮추는 것.
-------------------------
선형 SVM : 직선으로 선을 나눈것
비선형 SVM(Kernel SVM) : 차원을 추가해서 선형적으로 분리할수없는 애들을 선형적으로 분리한 후, 다시 원래상태로 복구시킴.

파생변수를 하나 더 만듬.
예)X2=X1^2 으로.

x2 x1 y

1차원에서의 선형은 점
2차원에서의 선형은 직선
3차원에서는 평면

감마
오버피팅이면 값을 작게. 언더피팅이면 값을 크게.
디폴트 1

SVM 모델링
- 연속형이면 피쳐 스케일링
- 범주형이면 원핫 인코딩

overfitting이면 c값을 낮춤. 규제를 강하게. 모델 복잡도 낮춤.

--------------
import pandas as pd
import numpy as np
#C값이 초반에 너무 천천히 변화해서 그래프가 요상하게 나오므로 로그를 씌움
df = pd.DataFrame({
    "C":np.log10(C_list),
    # "C": C_list,
    "Train": train_acc_list,
    "Test": test_acc_list
})
df.set_index("C")
-------------------
원래는 감마값만 바꾸는게 아니고 C값도 바꿔서 튜닝해야 함.

규제 파라미터는 알아야됨.
-------------------
08_지도학습

K-NN(K-Nearest Neighbors)
운영모델로서는 좋지 않음.
추론시간이 오래걸림. -> 서비스시간이 느림.
K값 설정:주위 몇개값을 볼거냐

overfitting 발생 = 모델복잡도 높고 = 이상치 많음

K=1으로하면 이상치를 가져왔을 때, 틀릴 가능성이 높음. overfitting 발생 가능성 높음.
K가 낮을수록 복잡도 높고 이상치 많음

K가 높을수록 복잡도 낮고 이상치 적음.

또 K가 너무 높으면 관련 없는 이상한 값을 가져올수도 있다.(underfitting 발생)

회귀형
x값에 해당하는 y값들의 평균을 가져옴.
분류형
y값들의 최빈값

거리 재는 방법 (a1,a2) ( b1, b2)
p=2면 유클리디안 거리 이용. (a점-b점)^2 하고 루트. ( (a1-b1)^2+(a2-b2)^2 ) 에 루트. a점과 b점 사이의 거리.
p=1이면 맨하탄거리 이용. |a점-b점| . a점 위치 빼기 b점 위치 하고 절대값. |a1-b1| + |a2-b2| 각각의 축별 거리를 더함.

거리를 각각 다 재야되니까 시간이 많이 걸림. 거리계산하고 정렬하고 값이 작은거 순으로 찾아야 됨. 할 때마다 데이터 계산을 해야함. 

피쳐별로 다르기 때문에 피쳐 스케일링 필요.

KNN이용 모델링
전처리

이진트리-True False로 답할수있는 질문을 함. 먼저 물어보는게 가장 중요한거. 오차가 적은 질문으로 물어봐야함.
오버피팅이 발생하기 쉬운 모델. 규제 파라미터가 많다. 
오차가 없어질때까지 가지가 뻗어나감. 그래서 이상치에 반응하기 쉬움.

블랙박스모델:모델이 예측해주는데, 왜 그렇게 예측해주는지 모름. 근거를 모름. 예)머신러닝 모델
화이트박스모델: 왜 그렇게 예측해주는지 안다. 근거를 안다.

아래 둘다 디시전 트리 기반임.
랜덤 포레스트와 부스팅 모델을 많이 사용.

이진트리
각 단계를 노드 라고함.
맨 위 노드를 루트노드라고 함.
맨 마지막 끝 노드를 터미널노드라고 함. 전통적인 트리모델에서는 리프노드라고 함.
중간에 있는 노드는 디시전 노드.

하이퍼파라미터
max_depth : 질문을 몇단계까지만 내려가라
max_leaf_nodes : 리프 노드 갯수 제한. 디폴트 None(무제한)
min_samples_leaf : 리프노드가 가져야하는 최소한의 샘플 수
min_samples_split : 분할하기 위한 최소한의 샘플 수
max_features : 디폴트는 피쳐들을 다 쓰는거. 질문만들 때 피쳐를 몇개 쓸건지 지정하는거.
	log2는 나온수에서 반올림
	뎁스마다 랜덤하게 피쳐를 다시 뽑아냄. 갯수만 동일
criterion : 분기할지 말지 결정. 오차가 0이면 더 분기 안함. 오차 계산해주는 함수.
	분류(gini(디폴트), entropy):불순물계산할때 쓰는거.
	회귀:squared_error, absolute_error, firedman_mse, poisson

디시전 트리를 하면 피쳐의 중요도를 알 수 있다. 루트노드일수록 중요한 피쳐.
feature_importances_ : 피쳐별 중요도를 반환

디시전트리
	범주형: 라벨 인코딩,연속형: 피쳐 스케일링 안함. y값을 분류할수있는 패턴 찾기 어렵. y값은 1은 몇개없고 0이 많음.
선형모델
	값의 차이를 통해 A의특징, B의특징, C의 특징을 비교할 수 있다. ?
y를 예측할때 모든 피쳐를 다씀.
디시전 트리 제외 나머지는 모든 피쳐를 이용해 계산.
------------------
앙상블
베깅:모델은 같은데 다른 데이터. 
보팅:서로다른거 학습해서 결합. 같은데이터 다른 모델. 서로 다른 답이 학습됐을 때. 한놈이 틀리면 다른놈이 메꿔줌.

부스팅:협업관계. 100개 학습해야되면, 앞에놈이 일하면 그거받아서 뒤에놈이 일하고, 주루룩 이어져있음. 대용량데이터로 정형데이터를 추론할 때 이용.

랜덤포레스트 : 회귀문제면 평균. 투표해서 많은거로 결정. 모델은 동일한거쓰고 데이터를 다르게함. 
	fit(X,y) x_train에서 샘플링해서 일부만 학습시킴. 샘플링을 랜덤하게(중복허용).

랜덤포레스트 쓰는 이유
	train데이터중에 일부만 학습
	두번쨰, 세번째 일부만 학습
	개별모델이 데이터중에 특정 일부씩 각각 학습
	데이터 복잡도가 너무 클 때, underfitting 발생 -> 이때, 랜덤 포레스트로 나눠서 학습시킴. 
