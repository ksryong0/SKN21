하위 단어 토큰화:단어를 더 작은단위로 쪼개서 토큰화.
ex)coworker -> co work er

하위단어토큰화방법들
BPE : 빈도수

unigram : 어휘사전을 만들어놓고 빼 나감.
wordpiece : 빈도수 + 확률.

토큰화->어휘사전->토크나이저
NLP -> DL 모델

모델하고 토크나이저는 짝임. 토크나이저를 만들일은 없을거임.

---------
06
Feature Extractor Text to Vector변환이 중요함. Estimator는 별로 안중요.
RAG?

워드임베딩의 문제 : 단어에 대한 벡터가 고정됨. 같은 단어 다른 뜻인 경우에 같은 벡터만 반환해줌.
contextualized word embedding :워드임베딩에 문맥 정보도 같이 고려.

BoW:출현 빈도만 보겠다. 문맥 정보가 없는게 문제.

n_gram : 문맥도 고려할수있게되지만, 컬럼 갯수가 많아지게 됨.