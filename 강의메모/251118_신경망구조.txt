Dataset 생성
DataLoader 생성
모델 정의
모델객체, 손실함수, Optimizer 객체 생성
학습
최종 평가

학습할때랑 추론할 때 다르게 일을 해야 되는 애들이 있다.

어제한거
--------------------
이름에 Network가 들어가는 이유 : 연결한다는 뜻. 결국엔 함수임.

DNN(Deep Nearal Network). Feed Forward Network라고도 함. ANN(Artificial NN)라고도 함.

CNN(Convolutional NN)
RNN(Recurrent NN)
Transformer

unit : wx+b에서 출력된 값

Hidden Layer : 

xw+b를 계산하는 방식의 차이들. 다 가중합 계산
Fully Connected Layer : feature수에 따라 weight가 각각 있음. w1x1+w2x2+...+b
계산을 연결이라고 함.
Convolution Layer : w1 w2 w3 이렇게 몇개만 만들고 슬라이딩윈도우 방식으로 x를 이동시키면서 계산.
Recurrent Layer : w는 하나. x1w 를 계산해서, 두번째 x2w 계산시에 앞에서 계산한 걸 이용함.
x1*w+w*h1
h1(이전처리 결과. x1w)
x2w+wbh2
(h2=x1*w+w*h1)
어제의 처리결과도 같이 넣어서 순차적으로 계산.
Embedding Layer : wx+b 하는데, 학생이라는 단어를 n차원 벡터로 바꿈. 자연어를 숫자로 바꿔줌.
ReLu : activation함수. 활성함수. 뭔가 활성화시키는 함수는 아님. 

선형성만 있으면 레이어를 나눌 필요가 없음. 레이어를 나누려면 비선형적 특성이 있어야됨.
y^=xw1w2w3...
=xW

미분값중에 변화가 제일 큰게 0.25
기울기 소실(gradient vanishing) : ㅁ*sig*a*sig ... sigmoid함수를 계속 곱하면 0에 가까워져버림. gradient가 0이 되어버림.
sigmoid함수를 미분하면 0일 때 0.25로 가장 큼.

lr1은 그래디언트가 거의 0이 됨.
활성함수 : 출력함수. 

탄젠트 하이퍼볼릭:시그모이드보다는 나은데, 기울기 소실이 해결은 안됨. 잘 안씀.
RNN(80년대 모델)의 활성함수로 사용됨. 그래디언트 값이 시그모이드 보다는 개선됨. ReLu가 나오기 전임.

ReLU:계산량이 적음. 기울기 소실 문제 없음. 양수에서는 기울기가 1. 
Dying ReLU : 입력값이 음수면 0이라서, back propagation 들어가면 기울기가 0이 되어버림. 기울기 소실.
Leaky ReLU : 입력값이 음수일 때, 기울기를 만들어줌. 알파는 0에서 1사이. 기울기가 알파.

소프트맥스:큰애는 더 커지고, 작은애는 더 작아짐. 출력 결과로 씀. 후처리 단계에서도 씀.

분류에서 로그를 쓰는 이유:작은거도 크게 보기위해. 너무 큰건 작게.

이진 분류
BCELoss 함수 : 양성일 떄, 음성일 때 각각 분류됨

다중분류
다 곱해서 더함? 마이너스를 붙이는이유는 값이 마이너스로 나와서
정답일 확률만 남음. 결국 로그 로스를 계산.

y를 원핫인코딩 해줌. 그러고 크로스 엔트로피 계산.
y
0	[1 0 0]
2	[0 0 1]
2
1
1
0

x->모델->Linear
              모델이 예측한 확률로 바꿔주기 위해, Linear로 나온 결과를 Softmax함.

중요
정답:OHE
모델예측:Softmax
확률을 표현하는 값으로 출력이 바뀌어 있음. 이 출력으로 카테고리컬 크로스 엔트로피로 계산.
크로스엔트로피 로스 함수는 정답을 OHE 처리해줌. 결과에 소프트맥스도 처리해줌. 그러고 카테고리컬 크로스 엔트로피도 해줌. 크로스엔트로피 로스 함수에는 트레인 데이터셋을 그대로 넣어줘야 함.

Stochastic Gradient Decent
데이터 몇개만 뽑아서 계산하므로 이상치에 취약.

미니 배치 Stochastic Gradient Decent
랜덤하게 데이터 사이즈를 뽑아오는데, 지정한 적은 데이터 양만 가져옴. 이상치 영향을 최소화 함.
순서대로 정한 양만큼씩 쭉 가져옴.

옵티마이저
W.data = W.data-W.gradient*lr

SGD기반 옵티마이저
기본 옵티마이저의 gradient decent방식에 최적화를 위한 방법들을 추가.
Momentum : 가던 방향으로 감. 같은 방향으로 가면 쭉 잘감. 잘못된 방향으로 가는 크기를 줄여줌. 기존 경사하강법 공식은 그대로 이용하는데 살짝 개선.
학습률을 개선
Adagrad : 파라미터별로 학습률을 다르게 함. lr/(그래디언트 누적) <- 이 값이 점점 0에 가까워짐
RMSProp : 

Adam : 
-----------------------
06_dataset과 dataloader

raw data->dataset->
데이터셋에 데이터 요청시 주는게 좋은 이유:메모리 관리 측면에서 좋음. 배치사이즈만큼 올려둠.
데이터셋은 최상위클래스로서의 타입. 나머지는 데이터셋한테 상속받음.

빌트인 데이터셋 : torch.utils.data.Dataset을 상속