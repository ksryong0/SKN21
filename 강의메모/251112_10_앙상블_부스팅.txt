앙상블-부스팅 모델
앙상블:여러개의 모델을 묶어서 추론하는 방식
데이터복잡도보다 모델의 복잡도가 낮으면 underfitting 발생.
투표방식:개별모델을 만들어서 여러 추정기가 각각 결과를 내면 투표해서 최종결과를 냄.
	각자 틀린걸 나머지가 보완해줌. 서로 다른걸 추론하게 해야함.
부스팅:앞의 오차를 줄이는게 목적. 뒤로갈수록 점점 좋아짐. 성능이 안좋은 애들끼리 묶은거임.

--------------------------------------------------------------------------------------------------------------------
대량의 데이터에서 좋은 모델 = 복잡도가 높은 모델.
->복잡도가 낮은 데이터를 쓰면 언더피팅남

그라디언트 부스팅
개별적으로 썼으면 언더피팅이 났을, 성능이 좋지않은애들끼리 묶음.
병렬처리가 쉽지 않음. 대용량 데이터에서 씀.

평균을 계산.
Residual(잔차) = 실제값-평균

부스팅모델에서는
X:feature
y:residual

predicted weight = 평균 + (Learning Rate * Residual)
학습율(Learning Rate) : 곱하는값을 우리가 정함. 0~1 실수
많은 학습을 통해 조금조금씩 맞춰감.

평균 ---------------------------------------------------------------------------실제값
	     predicted weight1->predicted weight2->predicted weight3->실제값
이런 느낌으로 오차를 줄여나감.

성능이 너무 좋은애를 쓰면, 처음부터 트레인셋에 맞춰져서 오히려 overfitting이 날 확률이 높아짐.

학습율을 너무 작게하면 모델이 너무 많이 필요해짐.->언더피팅 발생가능성 올라감.
학습율을 너무 크게하면 overfitting발생가능성 올라감.
학습율이 성능에 영향을 크게 미침.

n_iter_no_change : 몇개까지 하고 성능개선 없으면 학습을 멈출지 정함.
5로 설정하면 5개 모델 통과할때까지 성능개선이 없으면 학습을 멈추겠다.
validation_fraction : 이 비율까지 좋아지지 않으면 학습을 멈춤
러닝 레이트랑 n_estimators가 성능에 영향을 가장 많이줌
max_depth를 보통 5가 넘지 않게 설정.
n_estimators : 갯수에 따라 학습시간, 추론시간이 달라짐. 메모리 크기, 가용시간을 고려하여 결정해야함.

-----------------------
import time
# print(time.time()) # time() # 1970/01/01 00:00:00 ~ 실행할 때 까지 몇초지났는지 반환.
# print(time.time_ns())# time() # 1970/01/01 00:00:00 ~ 실행할 때 까지 몇(나노)초지났는지 반환.

코드 실행시간을 알고싶을때
time함수 프린트문
코드
time함수 프린트문
--------------------
속도가 느림.
----------------------------------------------------------------------------------------------------
XGBoost
느린 그라디언트 부스팅의 속도를 개선하여 빨라짐.
오버피팅 제어하기 위한 하이퍼파라미터 추가.
c로만든걸 파이썬에서 구동하게 래핑함. 래퍼라고 함.
코드 돌려보니까 확실히 빠름.
SVM도 이정도 성능이 나옴. 굳이 쓸필요는 없는듯.
----------------------------------------------------------------------------------------------------
11_최적화-경사하강법

최적화
오차를 최소화하는 방법을 찾음. 오차를 구하는 함수가 필요
손실함수(loss function) : 오차를 구하는 함수
학습(fit)할 때 모델의 성능을 최적화시키고 싶음. 그럼 오차를 구하는 함수가 필요함.
손실함수 종류
분류(계산방법 : log loss -> -log(모델이 예측한 정답의 확률). x축 1 y축 0.001 이런식으로 결과가 보여서 로그 씀)
	다중:cross-entropy
	이진:binary cross-entropy
		정답에 모델을 곱해서 로그 씌움.

		양성			   음성
		정답*log(p)		+ (1-정답)log(1-p)
		p:양성일확률 
회귀
	MSE : (정답-예측)^2
		작은오차는 줄이고, 큰오차는 키워줌. 오차가 최대한 해도 1.0이다. 오차 계산이 제대로 안됨.
		그래서 로그에 넣음.
최적화 방법
최적화 함수 만들기:데이터가 많을수록 최적화함수 찾는 시간이 오래걸림. 데이터를 넣었을떄 함수가 나오면 그걸 쓸수 있음. 근데 그거자체가 안되는 애들이 있음
->위 상황에 사용하는 방식이 경사하강법임
경사하강법(딥러닝 모델에서 사용)

w-파라미터
y = wx
y를 y^ 이랑 비교.
오차 발생하면 w를 조정.
w를 5로하니까 오차 20
4로하니까 오차 30
오차가 커져서 반대방향인 w를 6으로하니까 오차 10
w를 7		오차 5
w를 8 	오차 3
w를 9		오차 2	최적
w 10		오차 4
w 11		오차 10
1. w를 랜덤하게 선택
2. 오차를 줄이는 방법으로 w를 바꿔가면서 테스트.
3. 가장 적은 오차가 나오는 w를 찾음
오차를 낮추는 방향을 찾기. 오차가 0이 안됨.(갖고있는 데이터의 한계)->그래서 오차가 제일 낮은 지점을 찾아야함
이걸 식으로, 알고리즘으로 만들어야함.
w변경->오차변경->델타 y/ 델타x : 변화율

변화율
x, y 방향같다->기울기 양수
x, y 방향 다르다->기울기 음수
0->불변

w가 바뀔 때, 
델타 오차/ 델타 w
if 음수 -> 반대
오차는 loss함수로 계산.

파라미터는 learning rate임.
Wnew=W-learning rate*(loss function을 w로 미분한 값)
방향을 잘못 잡으면 잘못갈 수 있다. 

학습율을 너무 작게하면 최적화 전에 멈춤. underfitting
학습율을 너무 크게하면 최적값을 지나가버림. 왔다갔다하면서 오차가 더 커짐. overfitting

--------------
12_선형모델_선형회귀

선형회귀 모델 : LinearRegression
->규제:Ridge, Lasso, Elastic Net

분류:LogisticRegression

LinearRegression
y=w1x1+w2x2+w3x3+w4x4...+b
x와 y의 선형적인 증감을 w로 표시.
x와 y의 관계가 선형적일 때 이용.

w의 절대값이 크면(0에서 멀어질수록) y값에 영향을 많이 미침.
w가 양수면 비례
w가 음수면 반비례
b(bias)가 있어서 y값이 0이 안됨.

LinearRegression
w:coef_
b:intercept_

다항회귀

x1	x2	x3
   	x1^2 	x1^3

각 피쳐들을 제곱하여 전처리.
w1x1^2+w2x2^2+w3x3^2

원래피쳐가 x1 x2면
폴리노미얼 2차원하면
x1^2 x2^2 x1x2 이렇게 세개가 더생김

리니어 regression에서 오버피팅 발생시 : w를 0에 가까운 값을 곱해주면 피쳐의 영향도가 줄어듬.
손실함수->오차를 계산하는 함수
손실함수 결과 + ?
손실함수 -> 오차 + ?

L2 규제(Ridge Regression) : 제곱해서 L2규제
손실함수 = MSE(w)+ 시그마뭐시기 wi제곱(강의자료 참고)
알파를 작게하면 규제가 약해짐(규제항의 영향력이 줄어듬)
알파를 크게하면 규제가 커짐(규제항의 영향력이 커짐)

Lasso 규제
위랑 식만 다른듯.
제곱항이 아니라서 L1규제

ElasticNet(L1규제 + L2규제)
L1규제와 L2규제의 비율을 정해서 처리.

라소는 trainset이 0이 될수있음.
릿지는 trainset이 0이 될수없음

오버피팅->규제 L1, L2
언더피팅->Polynomial Features

