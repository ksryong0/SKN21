Text to Vector : 임베딩이라고도 함.
텍스트의 의미를 포함하는 실수로 바꿔야 함.

토큰을 단어라고 많이 얘기함.
BoW:단어가 몇번 나오냐 카운트. 많이나오면 중요한 단어.
DTM (행:문서, 열:단어)
전체문서->단어들->

TDM(행:단어, 열:문서)
n-gram : 문맥적 의미를 넣으려고 이용.
n이 너무 크면 희소성의 문제, 컬럼 너무 커짐. n이 너무 작으면 단어간 관계성(문맥)이 표현안됨.

TF-IDF
적은 문서에 출현할수록 값이 커짐. 전체문서수/해당단어가 나오는 문서수
전체문서가 너무 많으면 결과값이 너무 커짐. 그래서 로그 씌움.
max_df:너무많이나오면 오히려 의미 없는 단어일 수 있다.

-----------------------
워드 임베딩

nn.Linear(10000:단어수, 100:임베딩벡터차원수)

wout = nn.Linear(100:임베딩벡터차원, 10000:단어수) # output layer
071
output = wout(hidden) # 중심단어의 logit
output.argmax(dim=-1) 

Context...
나는 어제 배를 먹었다. 배를 타고 갔다. 3배면 살게.
배가 종류별로 다 들어와서 학습이 되어버림.
어떤 배인지 모름.
문맥을 파악해서 구별하고 싶음.
문장 전체를 입력으로 받음.

지금은 트랜스포머를 많이씀.





12월 6일 낮
12월 7일 낮

12월 13일 14일
12월 21일
12월 28일
1월 11일
1월 18일
1월 25일
2월 1일