Text to Vector : 임베딩이라고도 함.
텍스트의 의미를 포함하는 실수로 바꿔야 함.

토큰을 단어라고 많이 얘기함.
BoW:단어가 몇번 나오냐 카운트. 많이나오면 중요한 단어.
DTM (행:문서, 열:단어)
전체문서->단어들->

TDM(행:단어, 열:문서)
n-gram : 문맥적 의미를 넣으려고 이용.
n이 너무 크면 희소성의 문제, 컬럼 너무 커짐. n이 너무 작으면 단어간 관계성(문맥)이 표현안됨.

TF-IDF
적은 문서에 출현할수록 값이 커짐. 전체문서수/해당단어가 나오는 문서수
전체문서가 너무 많으면 결과값이 너무 커짐. 그래서 로그 씌움.
