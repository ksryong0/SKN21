LLM은 자기가 모르는건(knowledge cutoff:학습한 시점 이후의 데이터는 없음) 정확히 답변하지 못함
1.파인튜닝:sLLM 적용. 특정 도메인에 대한 서비스로 이용. 큰 모델에는 기간별로 학습.
2.RAG

챗지피티 + 웹 검색 기능
자료를 추가해서 질문내용을 증강해서 프롬프트를 보강해서 생성함.

데이터수집->정제->불러오기->문서분할(LLM은 입력 토큰 제한이 있음. 많이 보내는데 돈이 듬. 필요한 데이터들만 뽑아낼 수 있음.)
청크가 임베딩모델을 거쳐 임베딩벡터로 저장됨. 
질문에 대한 답을 벡터디비에서 찾아서 해줘

데이터로더 종류들이 많음.
page_content
metadata
id

----------복습-----------

webbaseloader:잘 이용하면 크롤링 코드를 최소화 할 수 있다. url 여러개를 넣을 수 있다.	

recursive
쓸데없는 내용까지 가져오는 경우가 있다.

docling, unstruct... 뭐시기 : 여러가지 종류의 문서들을 형태에 맞춰서 읽어줌. 시간이 좀 걸리는데 성능이 좋음.

-------------------
poppler 설치
다운받고 압축풀고

고급시스템설정->환경변수

path = C:\Users\Playdata\Downloads\poppler-25.12.0\Library\bin

--------------
tesseract
exe파일 설치

C:\Program Files\Tesseract-OCR
------------------
chunking

어떻게 질문에 맞는 내용들만 추려서 자를지.
최대한 쓸데없는건 빼고 자르는게 좋음.

한 내용을 두개의 청크로 나누는게 가장 안좋은 경우. 특정 문자열(ex:엔터) 기준으로 잘라봄. 그랬더니 5글자밖에 안되는 경우도 생김. 그래서 청크 사이즈가 생김.

CahracterTextSplitter:엔터두개를 구분자로 사용하여 분리.
글자수가 (5글자 정도로 작아서) 청크사이즈보다 작으면 다음 청크를 묶어줌. 근데 합쳤을때 청크사이즈보다 크면 안 합침. 청크가 청크사이즈보다 큰 애는 그냥 그대로 씀.

청크오버랩:앞 청크의 뒷부분을 뒤 청크의 앞에다 붙임.

recursiveCharacterTextsplitter:구분자를 여러개 만듬. 단계별로 청킹함. 최대한 길이를 청크사이즈 이내로 들어갈 수 있도록 계속해서 맞춰줌. 모든 청크가 청크사이즈를 넘어가지 않도록 계속해서 잘라냄.




