{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c0614b",
   "metadata": {},
   "source": [
    "# Tensor 생성\n",
    "- 파이토치에서 데이터를 저장하는 자료구조\n",
    "- ndarray와 성격, 사용법이 유사하다.\n",
    "\n",
    "> **pytorch의 tensor는 숫자 데이터 타입만 지원한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d118ec",
   "metadata": {},
   "source": [
    "##  원하는 형태(shape) 텐서 생성\n",
    "- **torch.tensor(자료구조 \\[, dtype\\])**\n",
    "    - 지정한 dtype(Data type)에 맞는 Tensor객체를 생성해서 반환한다.\n",
    "      \n",
    "## 특정 타입의 Tensor를 직접 생성\n",
    "- torch.tensor()로 생성하면서 dtype을 지정하면 아래 타입의 Tensor객체가 생성된다.\n",
    "- 원하는 Type의 Tensor클래스를 이용해 직접 생성해도 된다.\n",
    "- **torch.FloatTensor(자료구조)**\n",
    "    - float32 타입 텐서 생성\n",
    "- **torch.LongTensor(자료구조)** \n",
    "    - int64 타입 텐서생성\n",
    "- 그외\n",
    "    - BoolTensor(bool), CharTensor(int8), ShortTensor(int16), IntTensor(int32), DoubleTensor(float64)\n",
    "    \n",
    "## tensor 상태 조회\n",
    "- **tensor.shape, tensor.size(\\[축번호\\])**\n",
    "    -  tensor의 shape조회\n",
    "- **tensor.dtype, tensor.type()**\n",
    "    - tensor 원소들의 데이터타입 조회\n",
    "    - dtype은 **data type**을 type()은 tensor **객체의 클래스 타입**을 반환한다.\n",
    "- **tensor.ndim, tensor.dim()**  : tensor 차원\n",
    "- **tensor.numel()**: 전체 원소 개수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f81163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f3b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 1.76s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip uninstall torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa2b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 157ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torchvision \u001b[2m(1.5MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torch \u001b[2m(230.2MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m torchvision\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m torch\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 8.44s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 2.52s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch==2.8 torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a312ec-094a-459d-9962-841d3decd8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 2, 3], dtype=np.int8)#\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a471610-e0d4-477c-bbe7-03f8a8d7202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch Data types\n",
      "float torch.float16 torch.float32 torch.float64 torch.float32 torch.float64\n",
      "int: torch.int8 torch.int16 torch.int32 torch.int64 torch.int16 torch.int32 torch.int64\n",
      "uint: torch.uint8 torch.uint16 torch.uint32 torch.uint64\n",
      "bool: torch.bool\n"
     ]
    }
   ],
   "source": [
    "print('torch Data types')\n",
    "print(\"float\", torch.float16, torch.float32, torch.float64, torch.float, torch.double)  #torch.float: float32\n",
    "print(\"int:\", torch.int8, torch.int16, torch.int32, torch.int64, torch.short, torch.int, torch.long)# 가능\n",
    "print(\"uint:\", torch.uint8, torch.uint16, torch.uint32, torch.uint64)\n",
    "print(\"bool:\", torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e0dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 2]) torch.Size([2, 2])\n",
      "0축 크기: 2 2\n",
      "type: torch.FloatTensor torch.float32\n",
      "차원크기: 2 2\n",
      "원소개수: 4\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "# a = torch.tensor([[1,2],[3,4]], dtype=torch.float32, device='cuda') # tensor를 GPU 메모리에 올린다.\n",
    "\n",
    "\n",
    "print(\"shape:\", a.shape, a.size())\n",
    "print(\"0축 크기:\", a.shape[0], a.size(0))\n",
    "print(\"type:\", a.type(), a.dtype)  # type(): Tensor 객체 타입. dtype: data type => 둘은 좀 다르다.\n",
    "print('차원크기:', a.dim(), a.ndim)\n",
    "print('원소개수:', a.numel())\n",
    "print(\"device:\", a.device)  # tensor를 다루는 processor(메모리위치) - cpu, cuda(gpu).  CPU면 RAM. CUDA면 gpu의 VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f077a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.tensor(range(10), dtype=torch.int8)\n",
    "print(aa)\n",
    "aa.type()\n",
    "aa.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e40a7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.float64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Float, Double(32, 64bit 실수)/Int, Long(32, 64 bit 정수) type Tensor\n",
    "# b = torch.tensor([1,3,7], dtype=torch.float32)\n",
    "b = torch.FloatTensor([1,3,7])  #float32\n",
    "print(b.dtype)\n",
    "c = torch.IntTensor([10,20,30])    # int32\n",
    "print(c.dtype)\n",
    "d = torch.DoubleTensor([1, 2, 3])  # float64\n",
    "print(d.dtype)\n",
    "e = torch.LongTensor([10, 20, 30]) # int64\n",
    "print(e.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bc602",
   "metadata": {},
   "source": [
    "## 동일한 값을 원소로 가지는 Tensor 생성\n",
    "- **torch.zeros(\\*size), zeros_like(텐서)**: 0으로 구성된 tensor 생성\n",
    "- **torch.ones(\\*size), ones_like(텐서)**: 1로 구성된 tensor생성\n",
    "- **torch.full(size, fill_value), full_like(텐서, fill_value)**: 지정한 값으로 구성된 tensor생성\n",
    "  - size \n",
    "    - shape을 지정한다.\n",
    "    - zeros, ones는 가변인자이므로 축별 size를 순서대로 서정\n",
    "    - full 은 tuple/list로 shape을 지정한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "801e4ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.Size([3, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[100, 100],\n",
       "        [100, 100],\n",
       "        [100, 100]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(3,2,3) # 3 x 2 x 3\n",
    "print(a.dtype, a.shape)\n",
    "torch.ones(2,3)  # 2 X 3\n",
    "torch.full([3,2], fill_value=100) # 3 x 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1697cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 배열과 동일한 shape의 tensor 생성.\n",
    "a = torch.tensor([[1, 2],[3, 4]]) # 2 x 2\n",
    "\n",
    "print(a.shape)\n",
    "b = torch.zeros_like(a)  # a와 같은 shape, dtype 으로 생성.\n",
    "# b = torch.ones_like(a)\n",
    "# b = torch.full_like(a, 20)\n",
    "print(b.shape, b.dtype)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01301e5c",
   "metadata": {},
   "source": [
    "## 동일한 간격으로 떨어진 값들로 구성된 배열생성\n",
    "- **torch.arange(start=0, end, step=1)** \n",
    "- **torch.linspace(start, end, steps,)** : steps - 원소개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9b9fd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  9,  8,  7,  6,  5,  4,  3,  2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)  # end. 0 ~ 10-1 1씩 증가\n",
    "torch.arange(0, 1, 0.1) # 0 ~ 1-0.1, 0.1 \n",
    "torch.arange(10, 1, -1) # 10 ~ 1-(-1), -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa883dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 10, 5) # 0 ~ 10, 5개원소. 마지막 값 포함.\n",
    "# torch.linspace(0, 1, 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd2a03",
   "metadata": {},
   "source": [
    "## 빈 tensor 생성\n",
    "- **torch.empty(\\*size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a865f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0246e-31, 7.3148e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3,2,7) # 3 X 2 X 7. 빈 값을 넣을수는 없으니 0에 가까운 숫자로 랜덤하게 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec59585",
   "metadata": {},
   "source": [
    "## 난수를 이용한 생성\n",
    "\n",
    "- **torch.rand(\\*size)**: 0 ~ 1사이 실수로 구성된 배열을 생성. 각 값은 균등분포를 따른다.\n",
    "- **torch.randn(\\*size)**: 표준정규분포(평균:0, 표준편차:1)를 따르는 실수로 구성된 배열 생성\n",
    "- **torch.randint(low=0, high, size)**: 지정한 범위의 정수로 구성된 배열 생성\n",
    "- **torch.randperm(n)**: 0 ~ n-1 사이의 정수를 랜덤하게 섞은 값을 원소로 가지는 배열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1448b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1, 3, 2, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)  # seed 설정\n",
    "torch.rand(1, 3, 5) # 100 x 3 x 5\n",
    "torch.randn(30, 3) # 30 X 3. 표준정규분포로 만듬\n",
    "torch.normal(mean=10, std=3, size=(2,5)) # 그냥 정규분포로 만듬. 평균 10 표준편차 3 갯수는 2x5 로 만듬\n",
    "torch.randint(1, 100, (3, 3, 6))  \n",
    "torch.randperm(5) # 0 ~ 99 를 섞어서 구성. 중복 없음. 마지막 값 제외."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "139bda87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 23, 62, 76, 63, 16, 56, 32, 60, 40, 42, 14, 91, 92, 36, 64, 29, 37,\n",
       "        73, 82, 74, 61, 96, 66, 87, 28, 10, 70, 22, 86, 77, 54, 93,  3, 30, 31,\n",
       "        90, 71, 69, 59, 83, 75, 24, 72,  4, 21, 46, 89, 47, 44, 38, 78, 58, 15,\n",
       "        48, 81, 99, 49,  7, 26, 55, 17, 43, 65, 98, 94, 57, 11, 53, 52, 95, 84,\n",
       "        35, 39,  8, 25, 79,  0, 34, 88,  9, 18, 80, 67, 68, 85, 50,  6, 51, 97,\n",
       "        12, 33, 45,  2, 27,  5, 41,  1, 19, 20])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(100)\n",
    "a\n",
    "idx = torch.randperm(100)\n",
    "idx\n",
    "b = a[idx]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e9b7c88-3511-4c26-8282-5fc9ad857b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(100)\n",
    "a\n",
    "# a 섞기\n",
    "# idx = torch.randperm(100) # 0 ~ 99 섞어서 반환. -> index\n",
    "# idx\n",
    "# b = a[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21cd7e-1f92-49ce-b0ae-54a2be6cd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8494a6a",
   "metadata": {},
   "source": [
    "## Tensor gpu/cpu 메모리로 옮기기\n",
    "\n",
    "- pytorch는 데이터셋인 tensor를 cpu메모리와 gpu 메모리로 서로 옮길 수 있다.\n",
    "    - 데이터에 대한 연산처리를 어디서 하느냐에 따라 메모리를 선택한다.\n",
    "    - 장치는 문자열로 설정한다.\n",
    "        - CPU 사용: \"cpu\"\n",
    "        - nvida GPU: \"cuda\"\n",
    "        - Apple m1: \"mps\"\n",
    "            - pytorch 1.12 부터 지원\n",
    "- 옮기기\n",
    "    - tensor 생성시 `device` 파라미터를 이용해 설정\n",
    "    - `tensor.to(device)`를 이용해 설정\n",
    "- 현재 실행환경에서 어떤 장비를 사용할 수 있는지 확인\n",
    "    - nvidia gpu 사용가능확인\n",
    "        - `torch.cuda.is_available()` - nvida gpu 사용가능 여부\n",
    "        - `torch.backends.mps.is_available()` - M1 사용가능 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bf69bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c7fce59-a925-45ae-b201-d9f39a443f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) cpu\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 3], dtype=torch.float32, device=device) #생성할 때 device 지정.\n",
    "print(t, t.device) # tensor가 어느 device에 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaee501-8187-4104-8dc0-3114a57dca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t.to(\"cpu\")  # 다른 device로 옮기기.\n",
    "print(t2, t2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9df51a",
   "metadata": {},
   "source": [
    "## tensor를 상수로 변환\n",
    "- torch.Tensor객체를 파이썬 상수로 변환.\n",
    "- tensor객체.item()\n",
    "    - Scalar(상수) 또는 원소가 하나인 tensor를 python 상수로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88d14aa5-a06a-4f25-bcec-b94c1d554de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e24dc70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10) 0\n",
      "10\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(10)  # 상수(scalar) => 0차원 Tensor\n",
    "print(a, a.ndim)\n",
    "print(a.item())\n",
    "print(type(a), type(a.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20]]) 2\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([[20]]) # 원소가 하나인 N차원 배열\n",
    "print(b, b.dim())\n",
    "print(b.item()) #원소가 하나인 배열(텐서) 변환 가능. 원소가 여러개면 사용불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb6f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = torch.tensor([1, 10, 100])\n",
    "print(c)\n",
    "# print(c.item()) #원소가 여러개일 경우 Exception발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.tensor([10], device='cuda')\n",
    "print(d)\n",
    "print(d.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8709f0b",
   "metadata": {},
   "source": [
    "## ndarray 호환\n",
    "\n",
    "- ndarray를 tensor로 생성\n",
    "    - **torch.tensor(ndarray)**\n",
    "    - **torch.from_numpy(ndarray)**\n",
    "- tensor를 ndarray로 변환\n",
    "    - **tensor.numpy()**\n",
    "    - tensor가 gpu에 있을 경우 cpu로 옮긴 뒤 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff396b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c1d1b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray -> tensor\n",
    "arr = np.arange(1,10)\n",
    "\n",
    "torch.tensor(arr, dtype=torch.float32)\n",
    "torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a5fa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4498,  0.4551,  0.7487],\n",
      "        [ 1.0209, -1.1459, -1.7887],\n",
      "        [ 0.8077, -0.6379, -0.6140]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.4498315 ,  0.455081  ,  0.74866587],\n",
       "       [ 1.0209157 , -1.1458894 , -1.7886838 ],\n",
       "       [ 0.80773103, -0.637942  , -0.6140363 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor -> ndarray\n",
    "t = torch.randn(3,3)\n",
    "print(t)\n",
    "t.to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.randn(2,2, device=\"cuda\")\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9d47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2.numpy() \n",
    "# t2.to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f4911",
   "metadata": {},
   "source": [
    "# 원소 조회및 변경 \n",
    "\n",
    "## indexing/slicing\n",
    "\n",
    "- 대부분 Numpy 와 동일\n",
    "    - **slicing에서 step을 <u>음수로 지정할 수 없다.</u>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2300078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5,   5,   8,  -2,   0,  -9,   1, -10,  -6,  -4,  -4,  -6,  -4,  -3,\n",
       "         -2,   2,  -4,  -8,   3,   4, -10,   2,   7,   3,   2,   7,   2,   8,\n",
       "         -2,  -5,  -4,  -6,  -2,   3,   7,   1, -10,   0,  -9,   4,   5,   3,\n",
       "         -8,   2,   4,  -6,  -4, -10,  -2,   4,   7, -10,   5,  -8,   9,   6,\n",
       "         -6,   0,   9,   2,  -3,  -1,   0, -10,  -8,  -3,  -4,  -7,   9,  -9,\n",
       "          4,   1,  -4,  -8,   8,   8,   6,  -7,  -4,  -6, -10,  -5,   3,  -4,\n",
       "          3,  -6,  -7,   8,  -1,  -3,   6,  -9,  -8,  -9,   8,  -4,   3,  -4,\n",
       "         -4,   4])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(-10, 10, (100, ))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a28537ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, -9,  4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]\n",
    "t[[1, 5, -1]] # 여러개 조회-> fancy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73763041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -4, -10,  -9,  -2,   5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:5]\n",
    "t[10:15]\n",
    "t[90:]\n",
    "t[3:30:3]\n",
    "\n",
    "####### step 음수 안된다. 그래서 reverse(역순 조회)가 안됨. reverse하려면 flip() 사용\n",
    "# t[10:1:-2]  \n",
    "t[1:10:2].flip(dims=(0,)) #flip 함수를 쓰면 인덱스의 앞 뒤를 뒤집는다. reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ecfc6db-5a07-4c50-954c-ec2691652958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5,  -2,  -9, -10,  -4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1:10:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f1a3e",
   "metadata": {},
   "source": [
    "## boolean index\n",
    "- 논리 연산은 &, |, ~ 을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81965586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 1, 2, 3, 4, 2, 7, 3, 2, 7, 2, 8, 3, 7, 1, 4, 5, 3, 2, 4, 4, 7, 5,\n",
       "        9, 6, 9, 2, 9, 4, 1, 8, 8, 6, 3, 3, 8, 6, 8, 3, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boolean index\n",
    "t[t > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49576bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 2, 3, 2, 2, 3, 1, 4, 3, 2, 4, 4, 2, 4, 1, 3, 3, 3, 4])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[(t > 0) & (t < 5)] # 한글 and 못씀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8f93620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 1, 2, 3, 4, 2, 7, 3, 2, 7, 2, 8, 3, 7, 1, 4, 5, 3, 2, 4, 4, 7, 5,\n",
       "        9, 6, 9, 2, 9, 4, 1, 8, 8, 6, 3, 3, 8, 6, 8, 3, 4])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boolean index 처리 함수.\n",
    "t.masked_select(t > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "28e13041-ca1b-4a4d-8bd7-a85c9b7cc832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5,   5,   8,  -2,   0,  -9,   1, -10,  -6,  -4,  -4,  -6,  -4,  -3,\n",
       "         -2,   2,  -4,  -8,   3,   4, -10,   2,   7,   3,   2,   7,   2,   8,\n",
       "         -2,  -5,  -4,  -6,  -2,   3,   7,   1, -10,   0,  -9,   4,   5,   3,\n",
       "         -8,   2,   4,  -6,  -4, -10,  -2,   4,   7, -10,   5,  -8,   9,   6,\n",
       "         -6,   0,   9,   2,  -3,  -1,   0, -10,  -8,  -3,  -4,  -7,   9,  -9,\n",
       "          4,   1,  -4,  -8,   8,   8,   6,  -7,  -4,  -6, -10,  -5,   3,  -4,\n",
       "          3,  -6,  -7,   8,  -1,  -3,   6,  -9,  -8,  -9,   8,  -4,   3,  -4,\n",
       "         -4,   4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03a01027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100,   5,   8,  -2,   0,  -9,   1, -10,  -6,  -4,  -4,  -6,  -4,  -3,\n",
       "         -2,   2,  -4,  -8,   3,   4, -10,   2,   7,   3,   2,   7,   2,   8,\n",
       "         -2,  -5,  -4,  -6,  -2,   3,   7,   1, -10,   0,  -9,   4,   5,   3,\n",
       "         -8,   2,   4,  -6,  -4, -10,  -2,   4,   7, -10,   5,  -8,   9,   6,\n",
       "         -6,   0,   9,   2,  -3,  -1,   0, -10,  -8,  -3,  -4,  -7,   9,  -9,\n",
       "          4,   1,  -4,  -8,   8,   8,   6,  -7,  -4,  -6, -10,  -5,   3,  -4,\n",
       "          3,  -6,  -7,   8,  -1,  -3,   6,  -9,  -8,  -9,   8,  -4,   3,  -4,\n",
       "         -4,   4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변경\n",
    "t[0] = 100\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49419f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1, 10).reshape(3,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67affa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t[ 0축, 1축 ]\n",
    "# t[1, 2].item()\n",
    "t[0, 2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15855bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[[1,0], [2,2]]  #(1,2), (0,2)\n",
    "# t[[첫번째값, 두번째값]-0축, [첫번째값, 두번째값]-1축]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cfb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02f14d8",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e70f8",
   "metadata": {},
   "source": [
    "## shape 변경\n",
    "- 원소의 개수가 변하는 shape으로는 reshape이 안된다.\n",
    "- tensor객체.reshape(\\*shape) / view(\\*shape) 이용\n",
    "    - 변환 후 값을 변경하면 원본 배열의 값도 같이 바뀐다.\n",
    " > tensor.clone(): tensor를 복제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71f88268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12]) torch.Size([3, 4]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2]) torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(12)\n",
    "a2 = a.reshape(3,4)\n",
    "a3 = a.reshape((3,2,2))\n",
    "a4 = a.reshape((3,2,-1))  #한 개 axis는 -1로 설정가능하고 그럼 계산해서 알아서 설정해 준다.\n",
    "a5 = a.reshape((-1,2,3))  #한 개 axis는 -1로 설정가능하고 그럼 계산해서 알아서 설정해 준다.\n",
    "print(a.shape, a2.size(), a3.shape, a4.shape, a5.shape)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6744974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12]) torch.Size([3, 4]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "a5 = a.view(3,4)\n",
    "a6 = a.view((3,2,2))\n",
    "a7 = a.view((3,2,-1))  #한개 axis는 -1로 설정가능\n",
    "print(a.shape, a5.size(), a6.shape, a7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ace33cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7004, 15.1000,  0.7173,  0.4575],\n",
      "        [ 0.4692,  0.1864,  0.3191,  0.8249],\n",
      "        [ 0.2995,  0.8105,  0.3017,  0.3836]])\n"
     ]
    }
   ],
   "source": [
    "# a5[0, 0] = 12.1\n",
    "a2[0, 1] = 15.1\n",
    "\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88561ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7004, 15.1000,  0.7173,  0.4575,  0.4692,  0.1864,  0.3191,  0.8249,\n",
      "         0.2995,  0.8105,  0.3017,  0.3836])\n"
     ]
    }
   ],
   "source": [
    "# tensor복사: clone() 메소드. \n",
    "# r이 a를 reshape 또는 view 한거면, a의 값을 바꾸면 r의 값도 바뀜. clone은 안바뀜.\n",
    "r = a.reshape(3, 4).clone()\n",
    "r[0,0] = 100.1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ab05006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5106, 0.0412, 0.4950, 0.4496],\n",
      "        [0.4551, 0.4000, 0.8942, 0.8690],\n",
      "        [0.1611, 0.7323, 0.1078, 0.0743]])\n",
      "tensor([[1.0010e+02, 4.1167e-02, 4.9501e-01, 4.4960e-01],\n",
      "        [4.5508e-01, 4.0005e-01, 8.9419e-01, 8.6899e-01],\n",
      "        [1.6112e-01, 7.3226e-01, 1.0781e-01, 7.4339e-02]])\n"
     ]
    }
   ],
   "source": [
    "r = a.reshape(3, 4)\n",
    "print(r)\n",
    "a[0] = 100.1\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "220d6190-0c95-46f8-80c7-626d4ade8b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100.1000,  15.1000,   0.7173,   0.4575],\n",
       "        [  0.4692,   0.1864,   0.3191,   0.8249],\n",
       "        [  0.2995,   0.8105,   0.3017,   0.3836]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c2fa4",
   "metadata": {},
   "source": [
    "## dummy 축 늘리기\n",
    "\n",
    "- None을 이용 (numpy의 newaxis 대신 None을 사용한다.)\n",
    "- unsqueeze(dim=축번호)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "356a078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2, 2]) torch.Size([1, 2, 2])\n",
      "torch.Size([2, 2, 1]) torch.Size([2, 2, 1])\n",
      "torch.Size([2, 1, 2, 1, 1]) torch.Size([2, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[10,20],[10,20]])\n",
    "print(a.shape)\n",
    "# (3,5,6) -> (3:5:6:None)\n",
    "\n",
    "a1, a2 = a[None, :], a.unsqueeze(dim=0) # dim=정수만가능\n",
    "print(a1.shape, a2.shape)\n",
    "\n",
    "a3, a4 = a[:, :, None], a.unsqueeze(dim=-1)   # a[:, None] # 0축 :, 1축 None, 2번축 지정안함 -> 원래대로 그래서 (2, 1, 2) 가됨.\n",
    "print(a3.shape, a4.shape)\n",
    "\n",
    "a5, a6 = a3[:,None,:,:, None],  a3.unsqueeze(dim=1)  # unsqueeze(): dim=정수만가능 (한번에 dummy축은 하나만 추가가능.)\n",
    "print(a5.shape, a6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947b6ac",
   "metadata": {},
   "source": [
    "## dummy 축 제거\n",
    "- squeeze(\\[dim=축번호\\]) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e40b9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4, 1, 5, 1])\n",
      "torch.Size([3, 4, 5])\n",
      "torch.Size([3, 4, 1, 5, 1])\n",
      "torch.Size([3, 4, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(3, 1, 4, 1, 5, 1)\n",
    "print(t.shape)\n",
    "\n",
    "r1 = t.squeeze()  #축을 명시 하지 않으면 모두 제거\n",
    "print(r1.shape)\n",
    "\n",
    "r2 = t.squeeze(dim=1) # 특정 axis 제거\n",
    "print(r2.shape)\n",
    "\n",
    "r3 = t.squeeze(dim=[1,3]) # 여러 axis의 dummy 축 제거\n",
    "print(r3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3018717",
   "metadata": {},
   "source": [
    "# tensor 합치기\n",
    "- 합치는 기준 축을 제외한 나머지 축의 size는 같아야 한다.\n",
    "torch.cat([tensorA, tensorB, ...], dim=0) : 기준축 default 는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7564c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.arange(10).reshape(2,5)\n",
    "b = torch.arange(10,20).reshape(2,5)\n",
    "c = torch.arange(20,30).reshape(2,5)\n",
    "d = torch.arange(10,19).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bddebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a633d695",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee08217",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b, c], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb2f33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14],\n",
       "        [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], axis=1) # dim 대신 axis사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f7277a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24],\n",
       "        [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19, 25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b, c], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82477d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14],\n",
       "        [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], axis=-1)  # -1: 마지막 axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f441999c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 5 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m a.shape, d.shape\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 합치는 기준축 이외의 축 size는 같아야 한다.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#dim=0, Error  1축 size가 달라서 Error\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 0. Expected size 5 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "a.shape, d.shape\n",
    "# 합치는 기준축 이외의 축 size는 같아야 한다.\n",
    "torch.cat([a, d])  #dim=0, Error  1축 size가 달라서 Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaa81f",
   "metadata": {},
   "source": [
    "# 값의 위치(index) 변경\n",
    "- tensor 원소의 index의 위치를 바꾼다.\n",
    "- `tensor.transpose(axis1, axis2)` \n",
    "    - 두 축의 자리만 변경 할 수 있다.\n",
    "- `tensor.permute(axis1, axis2, axis3, ..)`\n",
    "    - 두 개 이상의 축 자리를 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 0,  4,  8],\n",
      "         [ 1,  5,  9],\n",
      "         [ 2,  6, 10],\n",
      "         [ 3,  7, 11]],\n",
      "\n",
      "        [[12, 16, 20],\n",
      "         [13, 17, 21],\n",
      "         [14, 18, 22],\n",
      "         [15, 19, 23]]])\n",
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "y = X.transpose(1, 2)\n",
    "print(y.shape)  # 2,4,3 행렬이됨.\n",
    "print(y)\n",
    "z = X.permute(2, 0, 1) # 원래 2번축, 원래 0번축, 원래 1번축. 4 x 2 x 3 이 됨.\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe8a88",
   "metadata": {},
   "source": [
    "# tensor 연산 및 주요 함수\n",
    "\n",
    "## element-wise 연산\n",
    "- tensor와 상수 연산시, tensor와 tensor간 연산시 원소별로 처리한다.\n",
    "- 행렬곱 연산을 제외하고 tensor간 연산시 피연산자 tensor간에 shape이 같아야 한다.\n",
    "    - shape이 다를 경우 조건이 맞으면 broadcasting을 한 뒤에 연산한다. (size가 다른 축의 경우 한개의 피연산자 size가 1일 경우 복사하여 shape을 맞춘다.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d3d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "tensor([50, 51, 52, 53, 54])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(10).reshape(2,5)\n",
    "b = torch.arange(10,20).reshape(2,5)\n",
    "c = torch.arange(50, 55)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f4f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100, 101, 102, 103, 104],\n",
      "        [105, 106, 107, 108, 109]])\n",
      "tensor([[-100,  -99,  -98,  -97,  -96],\n",
      "        [ -95,  -94,  -93,  -92,  -91]])\n",
      "tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False]])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(a + 100)\n",
    "print(a - 100)\n",
    "print(a < 7)\n",
    "print(a[a < 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a5ed369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 12, 14, 16, 18],\n",
      "        [20, 22, 24, 26, 28]])\n",
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "print(a + b)\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46ceb3e6-ecd2-41b0-b3c8-c2e87e05171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.Size([5]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(), c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15256a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50, 52, 54, 56, 58],\n",
      "        [55, 57, 59, 61, 63]])\n"
     ]
    }
   ],
   "source": [
    "# broadcasting\n",
    "print(a + c)\n",
    "# c를 브로드캐스팅해서 a와 더함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729874b2",
   "metadata": {},
   "source": [
    "## 주요 연산함수\n",
    "\n",
    "### 주요 상수\n",
    "- torch.e: 자연상수 E\n",
    "- torch.pi: 원주율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22a50df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.718281828459045, 2.718281828459045, 3.141592653589793, 3.141592653589793)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.e, np.e, torch.pi, np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d7323",
   "metadata": {},
   "source": [
    "#### torch.nan, torch.inf\n",
    "- nan: Not a Number, 주로 결측치를 표현한다.\n",
    "- inf: infinit 무한. \n",
    "    - torch.inf: 양의 무한\n",
    "    - -torch.inf: 음의 무한\n",
    "- torch.isnan(tensor)\n",
    "    - 원소별 결측치 확인\n",
    "- torch.isinf(tensor)    \n",
    "    - 원소별 inf 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a91f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(torch.inf > 10000000000000000, torch.inf < 10000000000)\n",
    "print(-torch.inf < -1000000000000000000000, -torch.inf > 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0f9cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n",
      "tensor([False, False,  True, False, False])\n",
      "tensor([False, False, False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(torch.tensor(-1))) # nan (계산결과가 없으므로-없는값-nan 반환)\n",
    "print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])))  # nan 여부 확인\n",
    "print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))  # inf 여부 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdcb35",
   "metadata": {},
   "source": [
    "## 주요 연산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fc6ac19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -3, -2],\n",
      "        [-1,  0,  1],\n",
      "        [ 2,  3,  4]])\n",
      "tensor([[4, 3, 2],\n",
      "        [1, 0, 1],\n",
      "        [2, 3, 4]])\n",
      "tensor([[2.0000, 1.7321, 1.4142],\n",
      "        [1.0000, 0.0000, 1.0000],\n",
      "        [1.4142, 1.7321, 2.0000]])\n",
      "tensor([[1.8316e-02, 4.9787e-02, 1.3534e-01],\n",
      "        [3.6788e-01, 1.0000e+00, 2.7183e+00],\n",
      "        [7.3891e+00, 2.0086e+01, 5.4598e+01]])\n",
      "tensor([[1.3863, 1.0986, 0.6931],\n",
      "        [0.0000,   -inf, 0.0000],\n",
      "        [0.6931, 1.0986, 1.3863]])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(-4, 5).reshape(3,3)\n",
    "print(x)\n",
    "print(torch.abs(x)) # 절대값 \n",
    "print(torch.sqrt(torch.abs(x))) #  제곱근\n",
    "print(torch.exp(x))  # torch.e**x\n",
    "print(torch.log(torch.abs(x)))\n",
    "print(torch.log(torch.exp(torch.tensor(1))))  # torch.log() 밑이 e인 로그계산\n",
    "print(torch.log10(torch.tensor(10)))         # torch.log10() 밑이 10인 로그계산\n",
    "print(torch.log2(torch.tensor(2)))           # torch.log2() 밑이 2인 로그계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "026192bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.8976, -1.3494, -1.2542],\n",
      "        [-1.6072, -1.2981,  0.6781],\n",
      "        [ 2.3251,  2.4113,  5.1126]])\n",
      "tensor([[-4., -1., -1.],\n",
      "        [-2., -1.,  1.],\n",
      "        [ 2.,  2.,  5.]])\n",
      "tensor([[-3.9000, -1.3500, -1.2500],\n",
      "        [-1.6100, -1.3000,  0.6800],\n",
      "        [ 2.3300,  2.4100,  5.1100]])\n",
      "tensor([[-4., -2., -2.],\n",
      "        [-2., -2.,  0.],\n",
      "        [ 2.,  2.,  5.]])\n",
      "tensor([[-3., -1., -1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [ 3.,  3.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "y = x + torch.randn((3,3))\n",
    "print(y)\n",
    "print(torch.round(y)) # 반올림\n",
    "print(torch.round(y, decimals=2)) # 소수점 둘째자리 이하에서 반올림\n",
    "print(torch.floor(y)) # 내림\n",
    "print(torch.ceil(y)) # 올림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358832d4",
   "metadata": {},
   "source": [
    "### 행렬곱\n",
    "- `@` 연산자 또는 `torch.matmul(tensor1, tensor2)` 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb349ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2],[3, 4],[5, 6]])\n",
    "\n",
    "y = torch.FloatTensor([[1, 2],[1, 2],])\n",
    "x.size(), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4283adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2])\n",
      "tensor([[ 3.,  6.],\n",
      "        [ 7., 14.],\n",
      "        [11., 22.]])\n",
      "tensor([[ 3.,  6.],\n",
      "        [ 7., 14.],\n",
      "        [11., 22.]])\n"
     ]
    }
   ],
   "source": [
    "z1 = x @ y\n",
    "z2 = torch.matmul(x, y)\n",
    "print(z1.shape, z2.shape)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch 행렬곱(Batch matrix muliplication) - bmm(). Batch처리:여러개를 일괄적으로 처리.\n",
    "# 피연산자로 두개의 3차원 tensor를 axis (1, 2)를 기준으로 행렬곱을 처리한다.\n",
    "import torch\n",
    "x = torch.FloatTensor(3,4,2)\n",
    "y = torch.FloatTensor(3,2,5)\n",
    "z = torch.bmm(x, y)\n",
    "z.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214bd73",
   "metadata": {},
   "source": [
    "## 기술통계함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11a0d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0191,  0.8030,  0.6913, -0.2228],\n",
      "        [-1.6799,  0.9245,  1.5985, -0.0509],\n",
      "        [ 0.1590,  0.2433, -1.6305,  0.4503]])\n"
     ]
    }
   ],
   "source": [
    "X=torch.randn(3,4)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b696371",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0892)\n",
      "tensor([-1.6627, -0.0068,  1.5802])\n",
      "tensor([[-1.6627],\n",
      "        [-0.0068],\n",
      "        [ 1.5802]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(X))  # default: 전체기준으로 계산. dim=None\n",
    "print(torch.sum(X, dim=1)) #dim/axis 지정: 지정한 axis의 index가 다른 값끼리 계산.\n",
    "print(torch.sum(X, dim=1, keepdims=True)) # 기술통계함수들을 실행하면 차원이 줄어든다. keepdims=True로 하면 차원유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9380, 0.4941],\n",
      "        [0.0620, 0.2007]])\n"
     ]
    }
   ],
   "source": [
    "# X=torch.rand(2,2)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ceaf6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2906,  0.7923, -0.7779])\n",
      "tensor([-1.5018,  1.9708,  0.6594,  0.1766])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(X, dim=1))\n",
    "print(torch.sum(X, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99c06f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1087)\n",
      "tensor([-0.5006,  0.6569,  0.2198,  0.0589])\n",
      "tensor([[-0.5006,  0.6569,  0.2198,  0.0589]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(X))\n",
    "print(torch.mean(X, dim=0))\n",
    "print(torch.mean(X, dim=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db8d8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9635)\n",
      "tensor(0.9283)\n",
      "tensor([1.0479, 0.1320, 2.7733, 0.1223])\n"
     ]
    }
   ],
   "source": [
    "print(torch.std(X)) # standard deviation 표준 편차\n",
    "print(torch.var(X)) # variance\n",
    "print(torch.var(X, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.메소드() \n",
    "print(X.sum(dim=1, keepdims=True))\n",
    "print(X.mean(dim=1, keepdims=True))\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72865f1-80e6-4433-a366-02449bc493df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1dbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.max(X,dim=0)\n",
    "v.values #max값\n",
    "v.indices #max값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afe123ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9380)\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9380, 0.4941]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(X)) # 전체 기준 max값 계산.\n",
    "print(torch.max(X, dim=0))  # 축을 지정하면 return_types.max 타입객체로 반환. max값과 max값의 index를 묶어서 반환\n",
    "# print(torch.max(X, dim=1))\n",
    "# print(torch.max(X, dim=1).values, torch.max(X, dim=1).indices, sep=\" || \")\n",
    "\n",
    "\n",
    "# print(torch.max(X, dim=0, keepdims=True))  #keepdims=True : 차원(rank)를 유지\n",
    "# print(torch.max(X, dim=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "234df8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.6799)\n",
      "torch.return_types.min(\n",
      "values=tensor([-1.6799,  0.2433, -1.6305, -0.2228]),\n",
      "indices=tensor([1, 2, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(X))\n",
    "print(torch.min(X, dim=0)) # return_types.min 타입객체로 반환. min값과 min값의 index를 묶어서 반환\n",
    "# print(torch.min(X, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33c01aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([2, 1, 1, 2])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.argmax(X))\n",
    "print(torch.argmax(X, dim=0)) # 각 열에서 가장 큰 값의 인덱스\n",
    "print(torch.argmax(X, dim=1)) # 각 행에서 가장 큰 값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642696a3-bfe3-41f5-a4fc-397075454e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.max(X, dim=0)\n",
    "print(a.values)\n",
    "print(a.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c5599ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bf59a",
   "metadata": {},
   "source": [
    "# autograd(자동미분)\n",
    "- 자동 미분을 이용해 gradient(미분계수)를 계산하는 pytorch system.\n",
    "- 딥러닝 모델에서 weight와 bias tensor들(Parameter)은 backpropagation(역전파)를 이용해 gradient를 구해서 loss가 줄어드는 방향으로 update를 하게된다.\n",
    "- pytorch는 이런 미분 수행을 자동으로 처리해 준다.\n",
    "    - gradient(기울기)를 구한다는 것은 미분을 한다는 것을 말한다.\n",
    "- tensor가 미분 가능하려면(gradient 계산 대상 변수) `requires_grad=True` 로 설정되 있어야 한다. (default: False)\n",
    "\n",
    "> - **미분**\n",
    ">   -  (순간) 변화율을 계산한다.\n",
    ">   - $$\\frac{\\partial y}{\\partial x}$$\n",
    ">       - x에 대한 y의 변화율. 즉 x가 변하면 y는 얼마나 변할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4148abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "변수 x의 data: tensor([1.])\n",
      "변수 x의 gradient값: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#gradient를 계산할 tensor는 requires_grad=True 설정. 딥러닝모델에서는 parameter들.\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "print(x.requires_grad)\n",
    "print(\"변수 x의 data:\", x.data)\n",
    "print(\"변수 x의 gradient값:\", x.grad)\n",
    "\n",
    "\n",
    "# y2 = torch.tensor(y1, requires_grad=True)\n",
    "# print(y2.requires_grad)\n",
    "# print(\"변수 x의 data:\", y2.data)\n",
    "# print(\"변수 x의 gradient값:\", y2.grad)\n",
    "# x.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "72d815df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x0000023B8E903580>\n",
      "tensor([1.]) tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#gradient를 계산할 tensor는 requires_grad=True 설정. 딥러닝모델에서는 parameter들.\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x ** 2 # y.grad_fn : 도함수를 저장\n",
    "y.backward() #y.grad_fn(x) gradient 값을 x.grad에 계산\n",
    "print(y.grad_fn)\n",
    "print(x.data, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변수 y의 data: tensor([1.])\n",
      "변수 y의 gradient값: <PowBackward0 object at 0x0000023B8EBC3490>\n",
      "None\n",
      "변수 y의 gradient값: tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# y는 requires_grad=True인 변수를 이용해서 계산. \n",
    "# 이계산을 처리하면서 자동으로 도함수도 같이 계산. 그 도함수를 y의 grad_fn에 저장\n",
    "# dy/dx 를 y.grad_fn에 저장\n",
    "\n",
    "# x의 변화에 따른 y의 변화율을 계산\n",
    "y = x ** 2\n",
    "y\n",
    "print(\"변수 y의 data:\", y.data)\n",
    "# print(y._grad_fn.item)\n",
    "print(\"변수 y의 gradient값:\", y.grad_fn)\n",
    "# y.grad_fn.\n",
    "print(y.backward())\n",
    "print(\"변수 y의 gradient값:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd542d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\Documents\\SKN21\\07_deeplearning_pytorch\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\Documents\\SKN21\\07_deeplearning_pytorch\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\Documents\\SKN21\\07_deeplearning_pytorch\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변수 y의 data: tensor([1.])\n",
      "변수 y의 gradient값: None\n"
     ]
    }
   ],
   "source": [
    "# y2 = torch.tensor(y, requires_grad=True)\n",
    "# print(y2.requires_grad)\n",
    "\n",
    "# x.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "80eed801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([1.])\n",
      "tensor([2.]) None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0],)\n",
    "result = x + 2* y #gradient 계산 대상: x. 연산:덧셈. 덧셈의 도함수를 result.grad_fn 저장.\n",
    "# result.grad_fn\n",
    "result.backward() # x의 gradient 계산. 그 결과를 x.grad에 저장.\n",
    "print(x.data, x.grad)\n",
    "print(y.data, y.grad)\n",
    "# result.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7cf9d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([1.])\n",
      "tensor([2.]) tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "result = x + 2 * y #gradient 계산 대상: x, y. 연산:덧셈. 덧셈의 도함수를 result.grad_fn 저장.\n",
    "result.grad_fn\n",
    "result.backward() # result에 대해 x, y의 gradient를 따로 계산. 그 결과를 x.grad, y.grad에 각각 저장.\n",
    "print(x.data, x.grad)\n",
    "print(y.data, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.]) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_23464\\81998953.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(y.data, y.grad)\n"
     ]
    }
   ],
   "source": [
    "# 합성함수 미분\n",
    "# 합성함수 - 함수(함수(함수(인자))). z = f(g(x))\n",
    "# 합성함수 z의 x에 대한 grad값 : g(x)의 grad값 x f(y)의 grad값\n",
    "# dz/dx = dz/dy * dy/dx\n",
    "a = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "y = a ** 2 + 3\n",
    "z = y ** 2 + b\n",
    "# print(y.grad_fn, z.grad_fn)\n",
    "z.backward()\n",
    "# dz/z(모든 requires_grad=True 변수)\n",
    "print(y.data, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "eeda5aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([16.])\n"
     ]
    }
   ],
   "source": [
    "print(a.data, a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d2d6db62-c2ba-46ca-a869-fe7ed47c3fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(b.data, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304c280",
   "metadata": {},
   "source": [
    "## torch.no_grad() \n",
    "- no_grad()를 with 구문에서 연산을 할 경우 with block 내에서 requires_grad=True로 설정된 변수를 이용한 계산식이 있더라도 grad_fn을 생성하지 않는다. 즉 gradient를 계산하지 않는다.\n",
    "- 딥러닝 모델을 평가 하거나 추론을할 때는 gradient를 계산할 필요가 없기 때문에 no_grad 구문을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9de65a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([3.0], requires_grad=True)\n",
    "# 도함수를 안쓰게 하려면\n",
    "with torch.no_grad():\n",
    "    z = a ** 2 # 계산할 때, 도함수도 찾음.\n",
    "    print(z.grad_fn)\n",
    "    print(a.grad_fn)\n",
    "    # z.backward() <- 얘도 안됨.\n",
    "# requires_grad=True 임에도 torch.no_grad 구문 안에서는 그래디언트 계산을 안함\n",
    "# 이후에 gradient를 구하려면 with문 밖에서 하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa3986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04d635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bea7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3811c4b",
   "metadata": {},
   "source": [
    "## gradient 값 초기화\n",
    "- backward() 에서 계산되어 저장된 gradient값을 초기화한다.\n",
    "- 반복 학습을 할 경우 gradient가 누적되는 문제가 발생한다. 그래서 한번 학습이 끝나면 다음 학습 전에 초기화하는 작업을 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "60622e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) None\n",
      "tensor([2.], grad_fn=<MulBackward0>)\n",
      "tensor([1.]) tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "print(x.data, x.grad)\n",
    "y = x ** 2\n",
    "print(y.grad_fn(x))\n",
    "y.backward()\n",
    "print(x.data, x.grad)\n",
    "x.grad = None # 그래디언트 초기화. 안그러면 다음 그래디언트 계산시, 앞에 그래디언트도 누적해서 더해버림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5d55ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "z = x ** 3\n",
    "z.backward()\n",
    "print(x.data, x.grad) # 아까 구한 gradient를 더함.\n",
    "x.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "60daa5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "k = x **4\n",
    "k.backward()\n",
    "print(x.data, x.grad)\n",
    "x.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b170c04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=2*3**2\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
