{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집에 도움이 되는 사이트\n",
    "\n",
    "- **국가 통계포털**\n",
    "    - https://kosis.kr\n",
    "    - 통계청에서 관리하는 공공데이터 포털로 다양한 카테고리의 국가 통계데이터를 제공한다.\n",
    "- **공공데이터 포털**\n",
    "    - https://www.data.go.kr\n",
    "    - 행정 안전부에서 제공하는 정부 데이터 포털\n",
    "- **Kaggle**\n",
    "    - https://kaggle.com\n",
    "    - 데이터과학 관련 경진대회 플랫폼\n",
    "    - 다양한 데이터들을 제공한다.\n",
    "- **구글 데이터셋 서치**\n",
    "    - https://datasetsearch.research.google.com\n",
    "    - 구글에서 제공하는 데이터셋 검색 사이트\n",
    "    - 키워드를 이용해 다양한 데이터셋을 검색하고 다운로드 받을 수 있다.\n",
    "- **AI Hub**\n",
    "    - https://aihub.or.kr\n",
    "    - 국내외 기관/기업에서 추진한 지능정보산업 인프라 조성사업에서 공개한 AI 학습용 데이터셋들을 제공한다.\n",
    "- **Roboflow Universe**\n",
    "    - https://universe.roboflow.com/\n",
    "    - Roboflow 라는 인공지능 회사에서 운영하는 데이터 저장소 사이트로 컴퓨터비전 관련 데이터셋을 주로 제공한다.\n",
    "- 기타\n",
    "    - **지자체**: 서울시 열린 데이터광장, 경기 데이터 드림\n",
    "    - **금융관련**: 한국거래소, 금융통계정보시스템등\n",
    "    - **영화관련**: 영화진흥위원회\n",
    "    - **대중교통**: 국가교통데이터베이스, 교통카드 빅데이터 통합정보시스템등    \n",
    "    - **관광관련**: 한국 관광 데이터랩등\n",
    "    - **날씨정보**: 기상청 기상자료 개방포털, 네이버 날씨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [크롬개발자 도구](https://developers.google.com/web/tools/chrome-devtools/)\n",
    "\n",
    "- 크롬 개발자 도구는 웹 개발 및 디버깅을 위한 강력한 도구로 크롬 웹브라우저에 내장되어 있다.\n",
    "    - `F12` 나 팝업 메뉴에서 `검사`를 선택한다.\n",
    "    - 엣지 브라우저도 같은 개발자 도구를 제공한다.\n",
    "- 웹 페이지의 HTML, CSS, JavaScript 코드를 검사하고 수정할 수 있으며, 네트워크 요청 응답 내용 분석, 성능 분석, 콘솔 로그 등 다양한 기능을 제공한다.\n",
    "- 주요 기능\n",
    "    - **요소 검사:** 웹 페이지의 특정 요소를 선택하여 HTML 구조, CSS 스타일, selector 등을 확인한다.\n",
    "    - **콘솔:** JavaScript 코드를 실행할 수 있고 Javascript 실행시 발생한 오류 메시지 등을 확인할 수 있다.\n",
    "    - **소스:** 웹 페이지의 JavaScript 코드를 확인할 수있고 디버깅을 위한 중단점(break point)를 설정하고 디버깅할 수 있다.\n",
    "    - **네트워크:** 웹 페이지를 요청할 때 발생하는 요청 및 응답 데이터를 분석하고 성능을 측정할 수 있다.\n",
    "    - **성능:** 웹 페이지의 로딩 시간, 렌더링 성능에 걸린 시간등을 분석할 수 있다.\n",
    "    - **애플리케이션:** 쿠키, 로컬 스토리지, 세션 스토리지 등 클라이언트 저장 데이터 확인 할 수 있다.\n",
    "- 개발자 도구는 크롤링 시 필수적인 도구이며, 수집할 페이지를 분석하는데 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- Markup 언어 parsing 라이브러리\n",
    "    - HTML이나 XML 문서 내에서 원하는 정보를 가져오기 위한 파이썬 라이브러리.\n",
    "- https://www.crummy.com/software/BeautifulSoup/\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- 설치\n",
    "    - beautifulsoup4 설치\n",
    "        - pip install beautifulsoup4\n",
    "    - lxml 설치(html/xml parser)\n",
    "        - pip install lxml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install  beautifulsoup4  lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 코딩 패턴\n",
    "1. 조회할 HTML내용을 전달하여 BeautifulSoup 객체 생성 \n",
    "1. BeautifulSoup객체의 메소드들을 이용해 문서내에서 필요한 정보 조회\n",
    "    - 태그이름과 태그 속성으로 조회\n",
    "    - css selector를 이용해 조회\n",
    "    - . 표기법을 이용한 탐색(Tree 구조 순서대로 탐색)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 객체 생성\n",
    "- BeautifulSoup(html str [, 파서])\n",
    "    - 매개변수\n",
    "        1. 정보를 조회할 html을 string으로 전달\n",
    "        2. 파서\n",
    "            - html.parser(기본파서)\n",
    "            - lxml : 매우 빠르다. html, xml 파싱 가능(xml 파싱은 lxml만 가능)\n",
    "                - 사용시 install 필요 \n",
    "                - `pip install lxml`\n",
    "                - install 후 커널 restart 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding='utf-8') as fr:\n",
    "    html_doc = fr.read()\n",
    "    \n",
    "print(html_doc[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 정보를 추출할 웹문서의 내용을 str으로 전달.\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서내에서 원하는 정보 검색\n",
    "\n",
    "### Tag 객체\n",
    "- 하나의 태그(element)에 대한 정보를 다루는 객체.\n",
    "    - BeautifulSoup 조회 메소드들의 **조회결과의 반환타입.**\n",
    "    - 조회 함수들이 찾은 Element가 하나일 경우 **Tag 객체를, 여러개일 경우 Tag 객체들을 담은 List(ResultSet)**를 반환한다.\n",
    "    - Tag 객체는 찾은 정보를 제공하는 메소드와 Attribute를 가지고 있다. 또 찾은 Tag가 하위 element를 가질 경우 찾을 수 있는 조회 메소드를 제공한다.\n",
    "- 주요 속성/메소드\n",
    "    - **태그의 속성값 조회**\n",
    "        - `Tag객체.get('속성명')`\n",
    "        - `Tag객체['속성명']`\n",
    "        - ex) `tag.get('href')` 또는 `tag['href']`\n",
    "    - **태그내 text값 조회**\n",
    "        - `Tag객체.get_text()`\n",
    "        - `Tag객체.text`\n",
    "        - ex) tag.get_text() 또는 tag.text\n",
    "    - **contents 속성**\n",
    "        - 조회한 태그의 모든 자식 요소들을 리스트로 반환\n",
    "        - ex) `child_list = tag.contents`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조회 함수\n",
    "- **태그의 이름으로 조회**\n",
    "    - `find_all()`\n",
    "    - `find()`\n",
    "- **css selector를 이용해 조회**\n",
    "    - `select(), select_one()`\n",
    "- **`.` 표기법(dot notation)**\n",
    "    - dom tree 구조의 계층 순서대로 조회\n",
    "    - 위의 두방식으로 찾은 tag를 기준으로 그 주위의 element 들을 찾을 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그의 이름으로 조회\n",
    "- **find_all**(name=태그명, attrs={속성명:속성값, ..})\n",
    "   - 이름의 모든 태그 element들을 리스트에 담아 반환.\n",
    "   - 여러 이름의 태그를 조회할 경우 List에 태그명들을 묶어서 전달한다.\n",
    "   - 태그의 attribute 조건으로만 조회할 경우 name을 생략한다. \n",
    "- **find**(name=태그명, attrs={속성명:속성값})\n",
    "    - 이름의 태그중 첫번째 태그 element를 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = soup.find_all(\"div\")   # 문서안에 있는 모든 div 요소(element)를 리스트로 반환.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://grandpark.seoul.go.kr/main/ko.do\">서울 대공원</a>,\n",
       " <a href=\"https://www.everland.com/web/everland/favorite/zootopia/index.html\">에버랜드</a>,\n",
       " <a href=\"https://www.coexaqua.com\">코엑스아쿠아리움</a>,\n",
       " <a href=\"http://www.naver.com\">네이버</a>,\n",
       " <a href=\"http://www.naver.com\">다음</a>,\n",
       " <a href=\"http://www.naver.com\">구글</a>,\n",
       " <a href=\"http://www.naver.com\">구글2</a>,\n",
       " <a href=\"http://www.naver.com\">구글3</a>,\n",
       " <a href=\"http://www.naver.com\">구글4</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(result))  # 조회 결과 개수\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: \n",
      "사자\n",
      "3마리\n",
      " \n",
      "사자\n",
      "3마리\n",
      "\n",
      "class속성값: ['animal_info'] ['animal_info']\n"
     ]
    }
   ],
   "source": [
    "tag1 = result[0]\n",
    "print(\"content:\", tag1.text, tag1.get_text())\n",
    "print(\"class속성값:\", tag1.get(\"class\"), tag1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = soup.find(\"div\")  # 조회결과 1개만(첫번째 것) 반환.\n",
    "print(type(result))\n",
    "print(\"-\"*50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그의 content와 attribute 조회\n",
    "\n",
    "print(\"content text:\", result.text)\n",
    "print(\"content text:\", result.get_text())\n",
    "print(\"attribue의 value:\", result.get(\"class\"))\n",
    "print(\"attribue의 value:\", result[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그의 모든 자식 요소들 조회\n",
    "result.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://www.everland.com/web/everland/favorite/zootopia/index.html\">에버랜드</a>]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "result = soup.find_all(\"a\")  # 태그 이름\n",
    "result = soup.find_all([\"a\", \"span\"])  #한번에 여러이름의 태그드을 조회.\n",
    "result = soup.find_all(\"div\", attrs={\"class\":\"name\"}) # 태그이름 + 속성\n",
    "result = soup.find_all(\"div\", attrs={\"class\":\"animal_info\", \"id\":\"animal1\"}) # 속성 조건이 여러개\n",
    "result = soup.find_all(\"a\", attrs={\"href\":\"https://www.coexaqua.com\"})\n",
    "\n",
    "import re\n",
    "result = soup.find_all(\"a\", attrs={\"href\":re.compile(r\"\\.html$\")}) # 정규표현식-.com으로 끝나는.\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = soup.find_all(\"a\")\n",
    "\n",
    "result_list = []\n",
    "for tag in result:\n",
    "    print(tag.text, tag['href'])\n",
    "    result_list.append([tag.text, tag['href']]) # list[text, href]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selector를 이용해 조회\n",
    "- **select(selector='css셀렉터')**\n",
    "    - css 셀렉터와 일치하는 tag들을 반환한다.\n",
    "- **select_one(selector='css셀렉터')**\n",
    "    - css 셀렉터와 일치하는 tag를 반환한다.\n",
    "    - 일치하는 것이 여러개일 경우 첫번째 것 하나만 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"example.html\", \"rt\", encoding=\"utf-8\") as fr:\n",
    "    html_doc = fr.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://grandpark.seoul.go.kr/main/ko.do\">서울 대공원</a>,\n",
      " <a href=\"https://www.everland.com/web/everland/favorite/zootopia/index.html\">에버랜드</a>,\n",
      " <a href=\"https://www.coexaqua.com\">코엑스아쿠아리움</a>,\n",
      " <a href=\"http://www.naver.com\">네이버</a>,\n",
      " <a href=\"http://www.naver.com\">다음</a>,\n",
      " <a href=\"http://www.naver.com\">구글</a>,\n",
      " <a href=\"http://www.naver.com\">구글2</a>,\n",
      " <a href=\"http://www.naver.com\">구글3</a>,\n",
      " <a href=\"http://www.naver.com\">구글4</a>]\n"
     ]
    }
   ],
   "source": [
    "# css selector를 이용한 조회\n",
    "\n",
    "result = soup.select(\"a\")         #  태그이름(a)  \n",
    "result = soup.select(\"a, span, img\") # 태그이름(여러개)\n",
    "result = soup.select(\"ul a\")    # ul의 자손인 a태그 찾는다.\n",
    "result = soup.select(\"ul > a\")  # ul의 자식인 a태그 찾기. ul 바로 밑에가 a 태그.\n",
    "\n",
    "# result = soup.select_one(\"#animal1\")       # *#animal1   # 모든 태그중 id=animal1\n",
    "# result = soup.select_one(\"a#animal1\")      # a#animal1   # a태그 중에서 id=animal1\n",
    "\n",
    "# result = soup.select(\"ul + div\")                 # ul의 다음 형제 태그중 div\n",
    "result = soup.select(\"body > div:nth-child(3)\")  # body의 3번째 자식 div. div중에서가 아님\n",
    "\n",
    "result = soup.select(\"a[href]\")                        # href 속성이 있는 a 태그들\n",
    "# result = soup.select(\"a[href='http://www.naver.com']\") # href='http://www.naver.com' 속성을 가진 a 그그\n",
    "# result = soup.select('a[href$=\".do\"]')                 # $=  href 속성값이 .do로 끝나는 a태그들\n",
    "# result = soup.select('a[href^=\"https\"]')               # =^  href 속성값이 https로 시작하는 a태그\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조회행수: 8\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xec in position 0: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# # csv 파일로 저장.\u001b[39;00m\n\u001b[32m     31\u001b[39m     result_df.to_csv(file_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m states = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m states.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\geopandas\\io\\file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\geopandas\\io\\file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyogrio\\geopandas.py:275\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    274\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyogrio\\raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1404\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1062\u001b[39m, in \u001b[36mpyogrio._io.get_features\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:926\u001b[39m, in \u001b[36mpyogrio._io.process_fields\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_ogr.pyx:28\u001b[39m, in \u001b[36mpyogrio._ogr.get_string\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'cp949' codec can't decode byte 0xec in position 0: illegal multibyte sequence",
      "decoding with 'cp949' codec failed"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# 셀렉트 쿼리\n",
    "sql = \"SELECT * FROM (SELECT * FROM charging_station WHERE objt_id IN (SELECT min(c1.objt_id) FROM charging_station c1 INNER JOIN charging_station c2 USING(x,y) group by x, y)) as c3 JOIN (SELECT parkinglot_id, count(objt_id) as 'charger_count' FROM elecar_parking.charging_station GROUP BY parkinglot_id) as c4 USING(parkinglot_id)\" \n",
    "with pymysql.connect(host=\"192.168.0.37\", port=3306, user='project1', password='1111', db='elecar_parking') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        result = cursor.execute(sql)\n",
    "        print(\"조회행수:\", result)\n",
    "        resultset = cursor.fetchall()\n",
    "\n",
    "# csv 저장\n",
    "if __name__ == \"__main__\":\n",
    "    result = resultset\n",
    "    # print(result)\n",
    "\n",
    "    # 저장할 디렉토리를 생성\n",
    "    save_dir = \"daum_news_list\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # dir 생성\n",
    "\n",
    "    # # 저장할 파일명 - 특정 기간마다 크롤링 수행할 경우 실행 날짜/시간을 이용해서 만들어 준다.\n",
    "    d = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    # print(d)\n",
    "    file_path = f\"{save_dir}/{d}.csv\"\n",
    "    # # DataFrame 생성\n",
    "    result_df = pd.DataFrame(result, columns=['parkinglot_id', 'OBJT_ID', 'CHGER_ID', 'STAT_NM', 'CHGER_TY', 'USE_TM', 'ADRES', 'RN_ADRES', 'CTPRVN_CD', 'SGG_CD', 'EMD_CD', 'BUSI_NM', 'TELNO', 'X', 'Y', 'charger_count'])\n",
    "    # # csv 파일로 저장.\n",
    "    result_df.to_csv(file_path, index=False)\n",
    "\n",
    "states = gpd.read_file(file_path)\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in result:\n",
    "    if tag.name == \"a\":\n",
    "        print(tag.text, tag['href'], tag.name)  # tag객체.name : 태그 이름."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
