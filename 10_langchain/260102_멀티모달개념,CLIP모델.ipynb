{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7107f5fc",
   "metadata": {},
   "source": [
    "멀티모달?\n",
    "사람이 오감을 이용해 판단하는 것처럼 AI는 텍스트 뿐만 아니라, 이미지, 오디오 등도 입력으로 받는것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bccc7",
   "metadata": {},
   "source": [
    "CLIP 모델?\n",
    "텍스트와 이미지를 결합한 데이터를 사용. -> 그걸 학습함\n",
    "이미지와 이미지에 대한 설명 텍스트를 학습.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504ca12",
   "metadata": {},
   "source": [
    "파인튜닝 필요 X.\n",
    "이미지 임베딩, 텍스트 임베딩 -> 코사인 유사도로 잘 맞는 텍스트 선택.\n",
    "텍스트 설명만으로 이미지 분류 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ed77a",
   "metadata": {},
   "source": [
    "CLIP(Contrastive Language-Image Pre-training) 모델은 OpenAI가 개발한 멀티모달 AI 모델로, 이미지와 텍스트를 동일한 벡터 공간에 매핑하여 서로 관련성을 이해하도록 학습하며, 별도의 레이블링 없이도 인터넷의 대규모 이미지-텍스트 쌍 데이터로 사전 학습되어 제로샷(Zero-Shot) 분류 등 다양한 시각적 작업을 수행할 수 있는 강력한 전이 학습이 가능한 모델입니다. 이는 텍스트만으로 이미지를 이해하고, 이미지를 보고 텍스트를 생성하며, 이미지 검색 등 생성형 AI의 기반이 되는 핵심 기술입니다. \n",
    "\n",
    "CLIP의 주요 특징\n",
    "- 멀티모달 이해: 이미지와 텍스트를 동시에 이해하고 연결합니다.\n",
    "- 자연어 지도 학습: 대규모 이미지-텍스트 쌍(예: 웹상의 이미지와 캡션)을 활용해 훈련되며, 별도의 라벨링이 필요 없습니다.\n",
    "- 벡터 공간 매핑: 이미지와 텍스트를 같은 임베딩 공간(벡터)으로 변환하여, 유사한 의미를 가진 이미지와 텍스트는 가깝게, 관련 없는 것은 멀게 배치합니다.\n",
    "- 제로샷 능력: 특정 작업에 대한 추가 학습(파인튜닝) 없이도, 자연어 프롬프트만으로 새로운 이미지를 분류하고 인식할 수 있습니다.\n",
    "- 전이 학습: 사전 학습된 능력을 다양한 다운스트림 작업에 유연하게 적용할 수 있어, 다른 AI 모델(예: DALL-E)의 기반이 됩니다. \n",
    "\n",
    "작동 방식\n",
    "- 데이터 수집: 인터넷에서 수많은 이미지와 그에 해당하는 텍스트 설명을 쌍으로 수집합니다.\n",
    "- 인코딩: 이미지 인코더와 텍스트 인코더를 사용해 각각의 이미지와 텍스트를 벡터(숫자 배열)로 변환합니다.\n",
    "- 대조 학습: 관련 있는 이미지-텍스트 쌍의 벡터는 서로 가깝게 만들고, 관련 없는 쌍의 벡터는 멀어지도록 모델을 학습시킵니다. \n",
    "\n",
    "활용 사례\n",
    "- 이미지 검색 (텍스트로 이미지 찾기).\n",
    "- 이미지 캡셔닝 (이미지에 맞는 텍스트 생성).\n",
    "- 제로샷 이미지 분류 (새로운 카테고리 이미지 분류).\n",
    "- 텍스트 기반 이미지 생성 모델(DALL-E, Stable Diffusion 등)의 핵심 요소. \n",
    "- 콘텐츠 조정 : CLIP은 부적절하거나 유해한 콘텐츠를 식별하고 필터링하기 위해 이미지와 함께 제공되는 텍스트를 분석하여, 온라인 플랫폼의 콘텐츠를 조정하는 데 사용할 수 있다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
