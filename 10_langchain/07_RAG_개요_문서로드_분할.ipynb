{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227316ea",
   "metadata": {},
   "source": [
    "# RAG(Retrieval Augmented Generation)\n",
    "- [RAG](https://python.langchain.com/v0.1/docs/modules/data_connection/)은 *Retrieval Augmented Generation*의 약자로, **검색 기반 생성 기법**을 의미한다. 이 기법은 LLM이 특정 문서에 기반하여 보다 정확하고 신뢰할 수 있는 답변을 생성할 수 있도록 돕는다.     \n",
    "- 사용자의 질문에 대해 자체적으로 구축한 데이터베이스(DB)나 외부 데이터베이스에서 질문과 관련된 문서를 검색하고, 이를 질문과 함께 LLM에 전달한다.\n",
    "- LLM은 같이 전달된 문서를 바탕으로 질문에 대한 답변을 생성한다. \n",
    "- 이를 통해 LLM이 학습하지 않은 내용도 다룰 수 있으며, 잘못된 정보를 생성하는 환각 현상(*hallucination*)을 줄일 수 있다.\n",
    "\n",
    "## RAG와 파인튜닝(Fine Tuning) 비교\n",
    "\n",
    "### 파인튜닝(Fine Tuning)\n",
    "\n",
    "- **정의**: 사전 학습(pre-trained)된 LLM에 특정 도메인의 데이터를 추가로 학습시켜 해당 도메인에 특화된 맞춤형 모델로 만드는 방식이다.\n",
    "- **장점**\n",
    "  - 특정 도메인에 최적화되어 높은 정확도와 성능을 낼 수 있다.\n",
    "- **단점**\n",
    "  - 모델 재학습에 많은 시간과 자원이 필요하다.\n",
    "  - 새로운 정보가 반영되지 않으며, 이를 위해서는 다시 학습해야 한다.\n",
    "\n",
    "### RAG\n",
    "\n",
    "- **정의**: 모델을 다시 학습시키지 않고, 외부 지식 기반에서 정보를 검색하여 실시간으로 답변에 활용하는 방식이다.\n",
    "- **장점**\n",
    "  - 최신 정보를 쉽게 반영할 수 있다.\n",
    "  - 모델을 수정하지 않아도 되므로 효율적이다.\n",
    "- **단점**\n",
    "  - 검색된 문서의 품질에 따라 답변의 정확성이 달라질 수 있다.\n",
    "  - 검색 시스템 구축이 필요하다.\n",
    "\n",
    "## 정리\n",
    "\n",
    "| 항목       | 파인튜닝 | RAG |\n",
    "| -------- | ---- | --- |\n",
    "| 도메인 최적화  | 가능   | 제한적 |\n",
    "| 최신 정보 반영 | 불가능  | 가능  |\n",
    "| 구현 난이도   | 높음   | 보통  |\n",
    "| 유연성      | 낮음   | 높음  |\n",
    "\n",
    "- LLM은 학습 당시의 데이터만을 기반으로 작동하므로 최신 정보나 기업 내부 자료와 같은 특정한 지식 기반에 접근할 수 없다.\n",
    "- 파인튜닝은 시간과 비용이 많이 들고 유지보수가 어렵다.\n",
    "-\t반면, RAG는 기존 LLM을 변경하지 않고도 외부 문서를 통해 그 한계를 보완할 수 있다.\n",
    "- RAG는 특히 빠르게 변화하는 정보를 다루는 분야(예: 기술 지원, 뉴스, 법률 등)에서 유용하게 활용된다. 반면, 정적인 정보에 대해 높은 정확도가 필요한 경우에는 파인튜닝이 효과적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017ca94-32e0-4460-8937-90a8d92ca07b",
   "metadata": {},
   "source": [
    "## RAG 작동 단계\n",
    "- 크게 \"**정보 저장(인덱싱)**\", \"**검색**, **생성**\"의 단계로 나눌 수 있다.\n",
    "  \n",
    "### 1. 정보 저장(인덱싱)\n",
    "RAG는 사전에 정보를 가공하여 **벡터 데이터베이스**(Vector 저장소)에 저장해 두고, 나중에 검색할 수 있도록 준비한다. 이 단계는 다음과 같은 과정으로 이루어진다.\n",
    "\n",
    "1. **Load (불러오기)**\n",
    "   - 답변시 참조할 사전 정보를 가진 데이터들을 불러온다.\n",
    "2. **Split/Chunking (문서 분할)**\n",
    "   - 긴 텍스트를 일정한 길이의 작은 덩어리(*chunk*)로 나눈다.\n",
    "   - 이렇게 해야 검색과 생성의 정확도를 높일 수 있다.\n",
    "3. **Embedding (임베딩)**\n",
    "   - 각 텍스트 조각을 **임베딩 벡터**로 변환한다.\n",
    "   - 임베딩 벡터는 그 문서의 의미를 벡터화 한 것으로 질문과 유사한 문서를 찾을 때 인덱스로 사용된다.\n",
    "4. **Store (저장)**\n",
    "   - 임베딩된 벡터를 **벡터 데이터베이스**(벡터 저장소)에 저장한다.\n",
    "   - 벡터 데이터베이스는 유사한 질문이나 문장을 빠르게 찾을 수 있도록 특화된 데이터 저장소이다.\n",
    "   \n",
    "![rag](figures/rag1.png)\n",
    "\n",
    "### 2. 검색, 생성\n",
    "\n",
    "사용자가 질문을 하면 다음과 같은 절차로 답변이 생성된다.\n",
    "1. **Retrieve (검색)**\n",
    "   - 사용자의 질문을 임베딩한 후, 이 질문 벡터와 유사한 context 벡터를 벡터 데이터베이스에서 검색하여 찾는다.\n",
    "2. **Query (질의 생성)**\n",
    "   - 벡터 데이터베이스에서 검색된 문서 조각과 사용자의 질문을 함께 **프롬프트**(prompt)로 구성하여 LLM에 전달한다.\n",
    "3. **Generation (응답 생성)**\n",
    "   - LLM은 받은 프롬프트에 대한 응답을 생성한다.\n",
    "   \n",
    "- **RAG 흐름**\n",
    "  \n",
    "![Retrieve and Generation](figures/rag2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e3d03-2250-4c79-aa83-7faf709ba4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f6fd91-e3de-4f9d-9c8a-9c21de7a768c",
   "metadata": {},
   "source": [
    "# Document Loader\n",
    "- LLM에게 질의할 때 같이 제공할 Data들을 저장하기 위해 먼저 읽어들인다.(Load)\n",
    "- 데이터 Resouce는 다양하다.\n",
    "    - 데이터를 로드(load)하는 방식은 저장된 위치와 형식에 따라 다양하다. \n",
    "      - 로컬 컴퓨터(Local Computer)에 저장된 문서\n",
    "        - 예: CSV, Excel, JSON, TXT 파일 등\n",
    "      - 데이터베이스(Database)에 저장된 데이터셋\n",
    "      - 인터넷에 존재하는 데이터\n",
    "        - 예: 웹에 공개된 API, 웹 페이지에 있는 데이터, 클라우드 스토리지에 저장된 파일 등\n",
    "\n",
    "![rag_load](figures/rag_load.png)\n",
    "\n",
    "- 다양한 문서 형식(format)에 맞춰 읽어오는 다양한 **document loader** 들을 Langchain에서 지원한다.\n",
    "    - 다양한 Resource들로 부터 데이터를 읽기 위해서는 다양한 라이브러리를 이용해 서로 다른 방법으로 읽어야 한다.\n",
    "    - Langchain은 데이터를 읽는 다양한 방식의 코드를 하나의 interface로 사용 할 수 있도록 지원한다.\n",
    "        - https://python.langchain.com/docs/how_to/#document-loaders\n",
    "    - 다양한 3rd party library(ppt, github 등등 다양한 3rd party lib도 있음. )들과 연동해 다양한 Resource로 부터 데이터를 Loading 할 수 있다.\n",
    "        - https://python.langchain.com/docs/integrations/document_loaders/\n",
    "- **모든 document loader는 기본적으로 동일한 interface(사용법)로 호출할 수있다.**\n",
    "- **반환타입**\n",
    "    - **list[Document]**\n",
    "    - Load 한 문서는 Document객체에 정보들을 넣는다. 여러 문서를 읽을 수 있기 대문에 list에 묶어서 반환한다.\n",
    "        - **Document 속성**\n",
    "            - page_content: 문서의 내용\n",
    "            - metadata(option): 문서에 대한 메타데이터(정보)를 dict 형태로 저장한다. \n",
    "            - id(option): 문서의 고유 id\n",
    "     \n",
    "- **주의**\n",
    "    - Langchain을 이용해 RAG를 구현할 때 **꼭 Langchain의 DocumentLoader를 사용해야 하는 것은 아니다.**\n",
    "    - DocumentLoader는 데이터를 읽어오는 것을 도와주는 라이브러리일 뿐이다. 다른 라이브러리를 이용해서 읽어 들여도 상관없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f25589-24a2-4f1f-9e8c-41e0594b6ce1",
   "metadata": {},
   "source": [
    "## 주요 Document Loader\n",
    "\n",
    "### Text file\n",
    "- TextLoader 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40998e95-a607-45d5-a3f6-2bb598b1aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "path = \"data/olympic.txt\"\n",
    "\n",
    "# 객체 생성 - 읽어들일 자원(파일)의 위치\n",
    "loader = TextLoader(path, encoding='utf-8')\n",
    "\n",
    "# Load - 읽어 오기\n",
    "docs = loader.load() # 메소드 호출시 읽는다.\n",
    "# loader.lazy_load() : 읽은 문서를 사용(조회)할 때 읽는다.\n",
    "\n",
    "print(type(docs), len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8523a-e87c-4214-aee9-c1aa8d1745c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdf1c6-436e-4026-a178-7c8b48c360ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2bbbd08-6afc-4f2f-985b-c96da7c3b943",
   "metadata": {},
   "source": [
    "### PDF\n",
    "- PyPDF, Pymupdf 등 다양한 PDF 문서를 읽어들이는 파이썬의  3rd party library들을 이용해 pdf 문서를 Load 한다.\n",
    "    - https://python.langchain.com/docs/integrations/document_loaders/#pdfs\n",
    "- 각 PDF Loader 특징\n",
    "    -  PyMuPDFLoader\n",
    "        -   텍스트 뿐 아니라 이미지, 주석등의 정보를 추출하는데 성능이 좋다.\n",
    "        -   PyMuPDF 라이브러리 기반\n",
    "    - PyPDFLoader\n",
    "        - 텍스트를 빠르게 추출 할 수있다.\n",
    "        - PyPDF2 라이브러리 기반. 경량 라이브러리로 빠르고 큰 파일도 효율적으로 처리한다.\n",
    "    - PDFPlumberLoader\n",
    "        - 표와 같은 복잡한 구조의 데이터 처리하는데 강력한 성능을 보여준다. 텍스트, 이미지, 표 등을 모두 추출할 수 있다. \n",
    "        - PDFPlumber 라이브러리 기반\n",
    "- 설치 패키지\n",
    "    - DocumentLoader와 연동하는 라이브러리들을 설치 해야 한다.\n",
    "    - `pip install pypdf -qU`\n",
    "    - `pip install pymupdf -qU`\n",
    "    - `pip install pdfplumber -qU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9647515-7c2d-447d-a4e4-d70a18e4e025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd7f13-c4d5-4db7-b5d7-27de7984624f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a703e-dd86-4cb5-82bf-8b6726aea0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b507b334-e33b-435f-8a50-7dd31fa6da6b",
   "metadata": {},
   "source": [
    "### Web 문서 로드\n",
    "\n",
    "#### WebBaseLoader를 이용해 Web 문서로딩\n",
    "\n",
    "requests와 BeautifulSoup을 이용해 web 페이지의 내용을 크롤링해서 Document로 loading한다.\n",
    "\n",
    "- 주요 파라미터\n",
    "  - **web_paths***: str | list[str]\n",
    "    - 크롤링할 대상 URL\n",
    "  - **requests_kwargs**: dict\n",
    "    - requests.get() 에 전달할 파라미터를 dict로 전달. (key: parameter변수명, value: 전달할 값)\n",
    "    - headers, cookies, verify 등 설정 전달\n",
    "  - **header_template**: dict\n",
    "    - HTTP Header 에 넣을 값을 dict 로 전달.\n",
    "  - **encoding**\n",
    "    - requests의 응답 encoding을 설정 (bs_kwargs의 from_encoding 보다 상위에서 적용됨)\n",
    "  - **bs_kwargs**\n",
    "    - BeautifulSoup initializer에 전달할 파라미터를 dict로 전달. (key: parameter변수명, value: 전달할 값)\n",
    "    -  주요 옵션\n",
    "       - **parse_only**: 요청 페이지에서 특정 요소만 선택해서 가져오기. **SoupStrainer를 사용**한다.\n",
    "         - BeautifulSoup의 `SoupStrainer` 를 이용해 페이지의 일부분만 가져오기\n",
    "           - 웹 페이지를 파싱(parse, 구조 분석)할 때, 페이지 전체가 아닌 특정 부분만 필요한 경우가 많다. BeautifulSoup 라이브러리의 SoupStrainer를 사용하면, 원하는 태그나 속성이 있는 요소만 골라서 파싱할 수 있다.\n",
    "           - BeautifulSoup(\"html문서\", parse_only=Strainer객체)\n",
    "               - Strainer객체에 지정된 영역에서만 내용 찾는다.\n",
    "           - `SoupStrainer(\"태그명\")`, `SoupStrainer([\"태그명\", \"태그명\"])`\n",
    "             - 지정한 태그 만 조회\n",
    "           - `SoupStrainer(name=\"태그명\", attrs={속성명:속성값})`\n",
    "             -  지정한 태그 중 속성명=속성값인 것만 조회\n",
    "        - **from_encoding**: Encoding 설정 \n",
    "          - \"from_encoding\":\"utf-8\"\n",
    "   - **bs_get_text_kwargs**:\n",
    "     - BeautifulSoup객체.get_text() 에 전달할 파라미터 dict로 전달. (key: parameter변수명, value: 전달할 값)\n",
    "     - **RAG 구축시 `separator` 와 `strip=True` 으로 설정하는 것이 좋다.** (RAG 품질을 위해 강력히 권장되는 설정이다.)\n",
    "       -  get_text() 는 기본적으로 태그를 제거하고 텍스트만 이어 붙여 반환한다. `separator=구분자문자` 를 지정하여 추출된 텍스트 요소들 사이에 원하는 구분자를 지정할 수있다. `\\n` 을 구분자로 사용하면 텍스트 블록 사이에 줄바꿈이 들어가 **문단의 구조를 어느정도 살릴 수 있다.**\n",
    "       -  웹 문서의 줄바꿈도 포함해서 읽기 때문에 공백과 줄바꿈이 혼재된 상태로 반환된다. `strip=True`로 설정하면 추출된 문자 앞뒤의 공백 문자들을 제거할 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98b0ba-97ac-445d-85c8-1e000299f926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b5826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73b892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf81342a",
   "metadata": {},
   "source": [
    "#### RecursiveUrlLoader\n",
    "\n",
    "- 주어진 URL에서 시작하여 그 페이지 안의 내부 링크를 재귀적으로 따라가며 여러 웹 문서를 자동 수집하여 로드한다.\n",
    "  - 시작 url을 요청/페이지를 파싱 한 뒤에 `<a href>` 들을 수집하고 그 페이지들을 요청/페이지 파싱을 한다. \n",
    "- WebBaseLoader가 단일 페이지(단일 URL) 단위라면 RecursiveUrlLoader는 **웹 사이트 구조 전체를 크롤링하는 전용 수집기**에 가깝다.\n",
    "```bash\n",
    "시작 URL\n",
    " ├─ 내부 링크 1\n",
    " │   ├─ 내부 링크 1-1\n",
    " │   └─ 내부 링크 1-2\n",
    " ├─ 내부 링크 2\n",
    " └─ 내부 링크 3\n",
    "```\n",
    "위 구조일때 무든 페이지를 재귀적으로 수집한다.\n",
    "- 주요 파라미터\n",
    "  - **url**: 시작 url\n",
    "  - **max_depth**\n",
    "    - 링크를 몇 단계 **깊이** 까지 따라갈지 제한\n",
    "    - 사이트 폭주를 막기 위한 안전장치\n",
    "      - **0**: 시작페이지만, **1**: 시작페이지 + 1차링크, **2**(기본값): 시작페이지 + 1차링크 + 2차링크\n",
    "  - **exclude_dirs**: list[str]\n",
    "    - 크롤링 제외 경로\n",
    "    - ex) `exclude_dirs=['/login', 'signup']`\n",
    "  - **prevent_outside**: bool\n",
    "    - True: base_url 바깥 링크는 가져오지 않고 무시한다.\n",
    "  - **base_url**: str\n",
    "    - prevent_outside=True일 때 바깥링크의 기준. 없으면 `url`(시작 url)의 host가 된다. \n",
    "  - **extractor**\n",
    "    - 문서 내용 추출 사용자 정의 함수\n",
    "    - default는 응답 받은 페이지를 `BeautifulSoup(응답페이지).get_text()` 로 텍스트를 추출한다.\n",
    "    - ````python\n",
    "    def custom_extractor(html:str) ->str:\n",
    "        # 웹 페이지 문서를 입력으로 받는다.\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        return soup.select_one('article').get_text() # 원하는 항목을 추출해서 반환한다.\n",
    "    \n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=start_url,\n",
    "        extractor=custom_extractor\n",
    "    )    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a6a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ac965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd26b68d",
   "metadata": {},
   "source": [
    "### ArxivLoader\n",
    "- https://github.com/lukasschwab/arxiv.py\n",
    "- [arXiv-아카이브](https://arxiv.org/) 는 미국 코렐대학에서 운영하는 **무료 논문 저장소**로, 물리학, 수학, 컴퓨터 과학, 생물학, 금융, 경제 등 **과학, 금융 분야의 논문**들을 공유한다.\n",
    "- `ArxivLoader` 를 사용해 원하는 주제의 논문들을 arXiv에서 가져와 load할 수 있다.\n",
    "- **arXiv API**를 사용해 논문을 가져올 수 있다.\n",
    "  - https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html\n",
    "- 설치\n",
    "  - `pip install langchain-community -qU`\n",
    "  - `pip install arxiv -qU`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cfa67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072b39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a984953e",
   "metadata": {},
   "source": [
    "### Docling\n",
    "- IBM Research에서 개발한 오픈소스 문서처리 도구로 다양한 종류의 문서를 구조화된 데이터로 변환해 생성형 AI에서 활용할 수있도록 지원한다.\n",
    "- **주요기능**\n",
    "  - PDF, DOCX, PPTX, XLSX, HTML, 이미지 등 여러 형식을 지원\n",
    "  - PDF의 **페이지 레이아웃, 읽기 순서, 표 구조, 코드, 수식** 등을 분석하여 정확하게 읽어들인다.\n",
    "  - OCR을 지원하여 스캔된 PDF나 이미지에서 텍스트를 추출할 수있다.\n",
    "  - 읽어들인 내용을 markdown, html, json등 다양한 형식으로 출력해준다.\n",
    "- 설치 : `pip install langchain-docling ipywidgets -qU` \n",
    "- 참조\n",
    "  - docling 사이트: https://github.com/docling-project/docling\n",
    "  - 랭체인-docling https://python.langchain.com/docs/integrations/document_loaders/docling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c0672-a932-4b55-bc39-1863e00ef3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e423ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639f1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cd786-947d-49ae-933c-627ca714c06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d0d3eb-8af4-432f-a207-728ff62358ee",
   "metadata": {},
   "source": [
    "### UnstructuredLoader\n",
    "- 다양한 비정형 문서들을 읽어 오는 Unstrctured 를 사용해, 다양한 형식의 문서들을 load 해 RAG, 모델 파인튜닝에 적용할 수있게 한다.\n",
    "  - 지원 파일 형식: \"csv\", \"doc\", \"docx\", \"epub\", \"image\", \"md\", \"msg\", \"odt\", \"org\", \"pdf\", \"ppt\", \"pptx\", \"rtf\", \"rst\", \"tsv\", \"xlsx\"\n",
    "- **다양한 형식의 파일로 부터 text를 로딩**해야 할 경우 유용하다. \n",
    "- Local에 library를 설치해서 사용하거나,  Unstructured 가 제공하는 API service를 사용할 수 있다.\n",
    "  - https://docs.unstructured.io\n",
    "- 텍스트 파일, PDF, 이미지, HTML, XML, ms-office(word, ppt), epub 등 다양한 비정형 데이터 파일을 처리할 수 있다.\n",
    "  - 설치, 지원 문서: https://docs.unstructured.io/open-source/installation/full-installation\n",
    "  - Langchain 문서: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
    "\n",
    "> - UnstructuredLoader PDF Load 시 Document 분할 기준\n",
    ">     -  문서의 구조와 콘텐츠를 기반으로 텍스트를 분할해 Document에 넣는다.\n",
    ">     -  분할 기준\n",
    ">        - 헤더(Header): 문서의 제목이나 섹션 제목 등\n",
    ">        - 본문 텍스트(NarrativeText): 일반적인 문단이나 설명문\n",
    ">        - 표(Table): 데이터가 표 형식으로 구성된 부분\n",
    ">        - 리스트(List): 순서가 있거나 없는 목록\n",
    ">        - 이미지(Image): 사진이나 그래픽 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87686d9-03d9-401a-9573-d57a2aacf965",
   "metadata": {},
   "source": [
    "#### 설치할 프로그램\n",
    "- poppler\n",
    "  - pdf 파일을 text로 변환하기 위해 필요한 프로그램\n",
    "  - windows: https://github.com/oschwartz10612/poppler-windows/releases/ 에서 최신 버전 다운로드 후 압축 풀어서 설치.\n",
    "    - 환경변수 Path에 \"설치경로\\Library\\bin\" 을 추가. (설치 후 IDE를 다시 시작한다.)\n",
    "  - macOS: `brew install poppler`\n",
    "  - Linux: `sudo apt-get install poppler-utils`\n",
    "- tesseract-ocr\n",
    "  - OCR 라이브러리로 pdf 이미지를 text로 변환하기 위해 필요한 프로그램 \n",
    "  - windows: https://github.com/UB-Mannheim/tesseract/wiki 에서 다운받아 설치. \n",
    "    - 환경변수 Path에 설치 경로(\"C:\\Program Files\\Tesseract-OCR\") 추가 한다. (설치 후 IDE를 다시 시작한다.)\n",
    "  - macOS: `brew install tesseract`\n",
    "  - linux(unbuntu): `sudo apt install tesseract-ocr`\n",
    "- 설치 할 패키지\n",
    "  - **libmagic 설치**\n",
    "      - windows: `pip install python-magic-bin -qU`\n",
    "      - macOS: `brew install libmagic`\n",
    "      - linux(ubuntu): `sudo apt-get install libmagic-dev`\n",
    "  - `pip install \"unstructured[pdf]\" -qU`\n",
    "      - 문서 형식별로 sub module을 설치한다. (pdf, docx ..)\n",
    "      - 모든 sub module 설치: `pip install unstructured[all-docs]`\n",
    "      - https://docs.unstructured.io/open-source/installation/full-installation\n",
    "  - `pip install langchain-unstructured -qU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a32d3-b64c-427f-8663-86e00ee88f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372606d9-8859-4d08-a488-dcd5e81fe9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4acfcbe1-cc26-4e87-8c41-d6fa7d461701",
   "metadata": {},
   "source": [
    "### Directory 내의 문서파일들 로딩\n",
    "- DirectoryLoader 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8eb4d8-3c1d-418d-a499-ee181d54b759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebc232-48ae-4cd2-ab3a-f6e26bd95ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1e8eb-2569-43f3-a458-dd020a322c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300bddc2",
   "metadata": {},
   "source": [
    "# Chunking (문서 분할)\n",
    "\n",
    "![rag_split](figures/rag_split.png)\n",
    "\n",
    "- Load 한 문서를 지정한 기준의 덩어리(chunk)로 나누는 작업을 진행한다.\n",
    "\n",
    "## 나누는 이유\n",
    "1. **임베딩 모델의 컨텍스트 길이 제한**\n",
    "    - 대부분의 언어 모델은 한 번에 처리할 수 있는 토큰 수에 제한이 있다. 전체 문서를 통째로 입력하면 이 제한을 초과할 수 있어 처리가 불가능해진다.\n",
    "2. **검색 정확도 향상**\n",
    "    - 큰 문서 전체보다는 특정 주제나 내용을 다루는 작은 chunk가 사용자 질문과 더 정확하게 매칭된다. 예를 들어, 100페이지 매뉴얼에서 특정 기능에 대한 질문이 있을 때, 해당 기능을 설명하는 몇 개의 문단만 검색되는 것이 더 효과적이다.\n",
    "    - 사용자 질문에 대해 문서의 모든 내용이 다 관련있는 것은 아니다. Chunking을 통해 가장 관련성 높은 부분만 선별적으로 활용할 수 있어 답변의 품질이 향상된다.\n",
    "    - 전체 문서에는 질문과 무관한 내용들이 많이 포함되어 있어 모델이 혼란을 겪을 수 있다. 적절한 크기의 chunk는 이런 노이즈를 줄여준다.\n",
    "3. **계산 효율성**\n",
    "    - 벡터 유사도 계산, 임베딩 생성 등의 작업이 작은 chunk 단위로 수행될 때 더 빠르고 효율적이다. 메모리 사용량도 줄일 수 있다.\n",
    "\n",
    "## 주요 Splitter\n",
    "- **Splitter**는 문서를 분할(chunking)을 처리해주는 도구들이다. Langchain은 분할 대상, 방법에 따라 다양한 splitter를 제공한다.\n",
    "- **Splitter 의 목표**\n",
    "  - 가능한 한 **의미 있는 덩어리를 유지**하면서, **최대 길이(chunk_size)**를 넘지 않도록 나누기.\n",
    "- https://reference.langchain.com/python/langchain_text_splitters/\n",
    "\n",
    "### CharacterTextSplitter\n",
    "가장  기본적인 Text spliter\n",
    "- 한개의 구분자를 기준으로 분리한다. (default: \"\\n\\n\")\n",
    "    - 분리된 조각이 chunk size 보다 작으면 다음 조각과 합칠 수 있다.\n",
    "        - 합쳤을때 chuck_size 보다 크면 안 합친다. chuck_size 이내면 합친다.\n",
    "    - 나누는 기준은 구분자이기 때문에 chunk_size 보다 글자수가 많을 수 있다.\n",
    "- chunk size: 분리된 문서(chunk) 글자수 이내에서 분리되도록 한다.\n",
    "    -  구분자를 기준으로 분리한다. 구분자를 기준으로 분리한 문서 조각이 chunk size 보다 크더라도 그대로 유지한다. 즉 chunk_size가 우선이 아니라 **seperator** 가 우선이다.\n",
    "- 주요 파라미터\n",
    "    - chunk_size: 각 조각의 최대 길이를 지정.\n",
    "    - seperator: 구분 문자열을 지정. (default: '\\n\\n')\n",
    "- CharacterTextSplitter는 단순 스플리터로 overlap기능을 지원하지는 않는다. 단 seperator가 빈문자열(\"\") 일 경우에는 overlap 기능을 지원한다. overlap이란 각 이전 청크의 뒷부분의 문자열을 앞에 붙여 문맥을 유지하는 것을 말한다.\n",
    "  \n",
    "### RecursiveCharacterTextSplitter\n",
    "- RecursiveCharacterTextSplitter는 **긴 텍스트를 지정된 최대 길이(chunk_size) 이하로 나누는 데 효과적인 텍스트 분할기**(splitter)이다.\n",
    "- 여러 **구분자(separators)를 순차적으로 적용**하여, 가능한 한 자연스러운 문단/문장/단어 단위로 분할하고, 최종적으로는 크기 제한을 만족시킨다.\n",
    "- 분할 기준 문자\n",
    "    1. 두 개의 줄바꿈 문자 (\"\\n\\n\")\n",
    "    2. 한 개의 줄바꿈 문자 (\"\\n\")\n",
    "    3. 공백 문자 (\" \")\n",
    "    4. 빈 문자열 (\"\")\n",
    "- 작동 방식\n",
    "    1. 먼저 가장 높은 우선순위의 구분자(\"\\n\\n\")를 기준으로 분리한다.\n",
    "    2. 분할된 조각 중 **chunk_size를 초과하는 조각**에 대해 다음 우선순위 구분자(\"\\n\" → \" \" → \"\")로 재귀적으로 재분할한다.\n",
    "    3. 이 과정을 통해 모든 조각(chunk)이 chunk_size를 초과하지 않도록 만든다.  \n",
    "- 주요 파라미터\n",
    "    - chunk_size: 각 조각의 최대 길이를 지정.\n",
    "    - chunk_overlap: 연속된 청크들 간의 겹치는 문자 수를 설정. 새로운 청크 생성 시 이전 청크의 마지막 부분에서 지정된 수만큼의 문자를 가져와서 새 청크의 앞부분에 포함시켜, 청크 경계에서 문맥의 연속성을 유지한다.\n",
    "      - 구분자에 의해 청크가 나눠지면 정상적인 분리이므로 overlap이 적용되지 않는다.\n",
    "      - 정상적 구분자로 나눌 수 없어 chunk_size에 맞춰 잘라진 경우 문맥의 연결성을 위애 overlap을 적용한다.\n",
    "    - separators(list): 구분자를 지정한다. 지정하면 기본 구분자가 지정한 것으로 변경된다.\n",
    "\n",
    "#### 메소드\n",
    "- `split_documents(Iterable[Document]) : List[Document]`\n",
    "    - Document 목록을 받아 split 처리한다.\n",
    "- `split_text(str) : List[str]`\n",
    "    - string text를 받아서 split 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a08d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"123456789012345678901234567890123456789012345678901234567890123456789\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "가나다라마바사아자차카타파하\n",
    "\n",
    "아야어여오요우유으이\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725f8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c8840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ae7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"1234567890123456789012345678901234567890\n",
    "12345678901234567890123456789\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "가나다라마바사아자차카타파하\n",
    "\n",
    "아야어여오요우유으이\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ RSTUVWXYZ\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b25cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8206f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed87439d",
   "metadata": {},
   "source": [
    "## Token 수 기준으로 나누기\n",
    "\n",
    "- LLM 언어 모델들은 입력 토큰 수 제한이 있어서 요청시 제한 토큰수 이상의 프롬프트는 전송할 수 없다.\n",
    "- 따라서 텍스트를 chunk로 분할할 때는 글자수 보다 **토큰 수를 기준으로 크기를 지정하는 것**이 좋다.  \n",
    "- 토큰기반 분할은 텍스트의 의미를 유지하면서 분할하는 방식이므로 문자 기반 분할과 같이 단어가 중간잘리는 것들을 방지할 수 있다. \n",
    "- 토큰 수 계산할 때는 사용하는 언어 모델에 사용된 것과 동일한 tokenizer를 사용하는 것이 좋다.\n",
    "  - 예를 들어 OpenAI의 GPT 모델을 사용할 경우 tiktoken 라이브러리를 활용하여 토큰 수를 정확하게 계산할 수 있다.\n",
    "\n",
    "### [tiktoken](https://github.com/openai/tiktoken) tokenizer 기반 분할\n",
    "- OpenAI에서 GPT 모델을 학습할 때 사용한 `BPE` 방식의 tokenizer. **OpenAI 언어모델을 사용할 경우 이것을 사용하는 것이 좀 더 정확하게  토큰dmf 계산할 수 있다.**\n",
    "- Splitter.from_tiktoken_encoder() 메소드를 이용해 생성\n",
    "  - `RecursiveCharacterTextSplitter.from_tiktoken_encoder()`\n",
    "  - `CharacterTextSplitter.from_tiktoken_encoder()`\n",
    "- 파라미터\n",
    "  - encode_name: 인코딩 방식(토큰화 규칙)을 지정. OpenAI는 GPT 모델들 마다 다른 방식을 사용했다. 그래서 사용하려는 모델에 맞는 인코딩 방식을 지정해야 한다.\n",
    "    - `cl100k_base`: GPT-4 및 GPT-3.5-Turbo 모델에서 사용된 방식.\n",
    "    - `r50k_base:` GPT-3 모델에서 사용된 방식 \n",
    "  - chunk_size, chunk_overlap, separators 파라미터 (위와 동일)\n",
    "- tiktoken 설치\n",
    "  - `pip install tiktoken`\n",
    "\n",
    "### HuggingFace Tokenizer\n",
    "- HuggingFace 모델을 사용할 경우 그 모델이 사용한 tokenizer를 이용해 토큰 기반으로 분할 한다.\n",
    "  - 다른 tokenizer를 이용해 분할 할 경우 토큰 수 계산이 다르게 될 수있다.\n",
    "- `from_huggingface_tokenizer()` 메소드를 이용.\n",
    "  - 파라미터\n",
    "    - tokenizer: HuggingFace tokenizer 객체\n",
    "    - chunk_size, chunk_overlap, separators 파라미터 (위와 동일)\n",
    "- `transformers` 라이브러리를 설치해야 한다.\n",
    "  - `pip install transformers` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99796ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a15cd4",
   "metadata": {},
   "source": [
    "## MarkdownHeaderTextSplitter\n",
    "- Markdown Header 기준으로 Splitter\n",
    "- Loading한 문서가 Markdown 문서이고 Header를 기준으로 문서의 내용이 나눠질때 사용.\n",
    "- https://reference.langchain.com/python/langchain_text_splitters/#langchain_text_splitters.MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "# 대주제1\n",
    "- 동물\n",
    "\n",
    "## 중주제1\n",
    "- 포유류\n",
    "\n",
    "- 조류\n",
    "\n",
    "### 소주제1\n",
    "- 개\n",
    "- 고양이\n",
    "- 까치\n",
    "- 독수리\n",
    "\n",
    "# 대주제2\n",
    "## 중주제2\n",
    "- 기차\n",
    "- 배\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583b342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
